{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ARPO UITARS 1.5 7B - OSWorld Inference\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/YOUR_REPO/blob/main/ARPO_UITARS_Inference.ipynb)\n",
        "\n",
        "This notebook demonstrates how to run inference with the ARPO-trained UITARS model on OSWorld tasks using **4-bit quantization** for memory efficiency.\n",
        "\n",
        "**Model**: [Fanbin/ARPO_UITARS1.5_7B](https://huggingface.co/Fanbin/ARPO_UITARS1.5_7B)\n",
        "\n",
        "**Performance**:\n",
        "- OSWorld (128 Tasks): **83.9%**\n",
        "- OSWorld Overall: **29.9%**\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Quick Start\n",
        "\n",
        "**This notebook works on:**\n",
        "- ‚úÖ **Google Colab** (Free T4 GPU - recommended!)\n",
        "- ‚úÖ **Local Jupyter** (8GB+ GPU)\n",
        "- ‚úÖ **Kaggle Notebooks**\n",
        "- ‚úÖ **Any Python environment** with GPU\n",
        "\n",
        "**No OSWorld setup required!** This notebook uses **real desktop screenshots** downloaded from the web, so you can start testing immediately.\n",
        "\n",
        "---\n",
        "\n",
        "## üìù What You'll Learn\n",
        "\n",
        "1. Load ARPO UITARS model with 4-bit quantization\n",
        "2. Process desktop screenshots\n",
        "3. Generate GUI actions (click, type, scroll, etc.)\n",
        "4. Handle multi-turn conversations\n",
        "5. Parse and execute actions\n",
        "\n",
        "Let's get started! üëá"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages - using latest versions for Qwen2.5-VL support\n",
        "%pip install -q --upgrade transformers accelerate\n",
        "%pip install -q qwen-vl-utils pillow torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model with 4-bit Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Logged in to HuggingFace\n",
            "\n",
            "ü§ñ Loading ARPO UITARS model with 4-bit quantization...\n",
            "This will take 1-2 minutes...\n",
            "‚úÖ CUDA available: Tesla T4\n",
            "üì¶ Loading processor...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e52d25a35dd949cbaed9d60be751d5ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/763 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1f7ccd5122b4c2084b68be647eedc9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b318929a15344e7abf2209e6a75cd589",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdfc0659b18f47ecae334dd09cb020e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cf433db193c46a98b6806e1602d386d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3322ce119c28446b810d76ab26ebbd56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd73cad05ffc43b5817a8efa3d7e933e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Processor loaded\n",
            "‚öôÔ∏è Configuring 4-bit quantization...\n",
            "üì• Loading model (this may take 1-2 minutes)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53c8e3f2a4084c75a48dbdc21757fceb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type qwen2_5_vl to instantiate a model of type qwen2_vl. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "================================================================================\n",
            "The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/python3.12/dist-packages/cv2/../../lib64')}\n",
            "The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "The following directories listed in your path were found to be non-existent: {PosixPath('https'), PosixPath('//mp.kaggle.net')}\n",
            "The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('http'), PosixPath('8013')}\n",
            "The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-3p9206ekdjht9 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true ')}\n",
            "The following directories listed in your path were found to be non-existent: {PosixPath('/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages')}\n",
            "The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "The following directories listed in your path were found to be non-existent: {PosixPath('/content/ARPO_UITARS_Inference-296ac34e-157b-4719-a514-26b8f6979a0b.ipynb')}\n",
            "The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "DEBUG: Possible options found for libcudart.so: {PosixPath('/usr/local/cuda/lib64/libcudart.so')}\n",
            "CUDA SETUP: PyTorch settings found: CUDA_VERSION=126, Highest Compute Capability: 7.5.\n",
            "CUDA SETUP: To manually override the PyTorch CUDA version please see:https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
            "CUDA SETUP: Required library version not found: libbitsandbytes_cuda126.so. Maybe you need to compile it from source?\n",
            "CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...\n",
            "\n",
            "================================================ERROR=====================================\n",
            "CUDA SETUP: CUDA detection failed! Possible reasons:\n",
            "1. You need to manually override the PyTorch CUDA version. Please see: \"https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
            "2. CUDA driver not installed\n",
            "3. CUDA not installed\n",
            "4. You have multiple conflicting CUDA libraries\n",
            "5. Required library not pre-compiled for this bitsandbytes release!\n",
            "CUDA SETUP: If you compiled from source, try again with `make CUDA_VERSION=DETECTED_CUDA_VERSION` for example, `make CUDA_VERSION=118`.\n",
            "CUDA SETUP: The CUDA version for the compile might depend on your conda install. Inspect CUDA version via `conda list | grep cuda`.\n",
            "================================================================================\n",
            "\n",
            "CUDA SETUP: Something unexpected happened. Please compile from source:\n",
            "git clone https://github.com/TimDettmers/bitsandbytes.git\n",
            "cd bitsandbytes\n",
            "CUDA_VERSION=126\n",
            "python setup.py install\n",
            "CUDA SETUP: Setup Failed!\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.integrations.bitsandbytes because of the following error (look up to see its traceback):\n\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1792\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/integrations/bitsandbytes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mbitsandbytes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcuda_setup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresearch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m from .autograd._functions import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/research/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m from .autograd._functions import (\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmatmul_fp8_global\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/research/autograd/_functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalOutlierPooler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMatmulLtState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_inverse_transform_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mundo_layout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOMPILED_WITH_CUDA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/cextension.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mCUDASetup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_log_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         raise RuntimeError('''\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mCUDA\u001b[0m \u001b[0mSetup\u001b[0m \u001b[0mfailed\u001b[0m \u001b[0mdespite\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0mbeing\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mrun\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfollowing\u001b[0m \u001b[0mcommand\u001b[0m \u001b[0mto\u001b[0m \u001b[0mget\u001b[0m \u001b[0mmore\u001b[0m \u001b[0minformation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3645789104.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Load model with quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üì• Loading model (this may take 1-2 minutes)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     model = Qwen2VLForConditionalGeneration.from_pretrained(\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mrepo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantization_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3669\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3670\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3671\u001b[0m                 \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             )\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_bnb_backend_availability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_bitsandbytes_multi_backend_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1779\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1781\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1782\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1793\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1795\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1796\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.bitsandbytes because of the following error (look up to see its traceback):\n\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ===== HuggingFace Authentication =====\n",
        "# Login to HuggingFace (required for gated models)\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Option 1: Login with token\n",
        "try:\n",
        "    # Get your token from: https://huggingface.co/settings/tokens\n",
        "    login(token=\"YOUR_HF_TOKEN\")  # ‚Üê Replace with your token!\n",
        "    print(\"‚úÖ Logged in to HuggingFace\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è HuggingFace login failed: {e}\")\n",
        "    print(\"Proceeding anyway (model might be public)...\")\n",
        "\n",
        "# Model configuration\n",
        "repo = \"Fanbin/ARPO_UITARS1.5_7B\"\n",
        "\n",
        "print(\"\\nü§ñ Loading ARPO UITARS model with 4-bit quantization...\")\n",
        "print(\"This will take 1-2 minutes...\")\n",
        "\n",
        "# Check if CUDA is available\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"‚ö†Ô∏è WARNING: CUDA not available. This will be very slow!\")\n",
        "    print(\"On Colab: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
        "    use_quantization = False\n",
        "else:\n",
        "    print(f\"‚úÖ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
        "    use_quantization = True\n",
        "\n",
        "# Load processor\n",
        "print(\"üì¶ Loading processor...\")\n",
        "processor = AutoProcessor.from_pretrained(repo, trust_remote_code=True)\n",
        "print(\"‚úÖ Processor loaded\")\n",
        "\n",
        "# Configure 4-bit quantization\n",
        "if use_quantization:\n",
        "    print(\"‚öôÔ∏è Configuring 4-bit quantization...\")\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "    \n",
        "    # Load model with quantization\n",
        "    print(\"üì• Loading model (this may take 1-2 minutes)...\")\n",
        "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "        repo,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "else:\n",
        "    # Load model without quantization (CPU fallback)\n",
        "    print(\"üì• Loading model in FP16 (no quantization)...\")\n",
        "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "        repo,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "print(f\"\\n‚úÖ Model loaded successfully!\")\n",
        "print(f\"üìç Device: {model.device}\")\n",
        "print(f\"üî¢ Dtype: {model.dtype}\")\n",
        "\n",
        "# Print memory usage\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Troubleshooting Model Loading\n",
        "\n",
        "If you encounter errors in the previous cell, try these solutions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üí° If you're still having issues:\n",
            "1. Restart runtime (Runtime ‚Üí Restart runtime)\n",
            "2. Clear outputs (Edit ‚Üí Clear all outputs)\n",
            "3. Run cells 1-4 again\n",
            "4. If still failing, try without quantization (see commented code above)\n"
          ]
        }
      ],
      "source": [
        "# ===== Troubleshooting Solutions =====\n",
        "\n",
        "# Problem 1: \"AttributeError: 'weight' is not an nn.Module\"\n",
        "# Solution: Restart runtime and reinstall packages\n",
        "\"\"\"\n",
        "# In Colab: Runtime ‚Üí Restart runtime\n",
        "# Then run these:\n",
        "%pip uninstall -y bitsandbytes\n",
        "%pip install bitsandbytes==0.43.0\n",
        "# Then re-run cells 1-4\n",
        "\"\"\"\n",
        "\n",
        "# Problem 2: Out of memory\n",
        "# Solution: Try without quantization or reduce memory usage\n",
        "\"\"\"\n",
        "# Option A: Load without quantization (needs more memory)\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Fanbin/ARPO_UITARS1.5_7B\",\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,  # or torch.float16\n",
        ")\n",
        "\n",
        "# Option B: Enable CPU offload\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Fanbin/ARPO_UITARS1.5_7B\",\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    offload_folder=\"offload\",\n",
        "    offload_state_dict=True,\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# Problem 3: CUDA not available\n",
        "# Solution: Enable GPU in Colab\n",
        "\"\"\"\n",
        "# Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí T4 GPU ‚Üí Save\n",
        "# Then Runtime ‚Üí Restart runtime\n",
        "# Re-run all cells\n",
        "\"\"\"\n",
        "\n",
        "# Problem 4: HuggingFace authentication\n",
        "# Solution: Login manually\n",
        "\"\"\"\n",
        "from huggingface_hub import login\n",
        "login()  # This will prompt for your token\n",
        "# Or use: login(token=\"YOUR_TOKEN_HERE\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"üí° If you're still having issues:\")\n",
        "print(\"1. Restart runtime (Runtime ‚Üí Restart runtime)\")\n",
        "print(\"2. Clear outputs (Edit ‚Üí Clear all outputs)\")\n",
        "print(\"3. Run cells 1-4 again\")\n",
        "print(\"4. If still failing, try without quantization (see commented code above)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üö® QUICK FIX: Load Model WITHOUT Quantization\n",
        "\n",
        "**Run this cell if you got bitsandbytes CUDA errors above!**  \n",
        "This skips quantization and loads the model in BFloat16 - works perfectly on Colab T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Logged in\n",
            "\n",
            "ü§ñ Loading model WITHOUT quantization (BFloat16)...\n",
            "\n",
            "üì¶ Loading processor...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db2c07a789d343068b168542360aaf54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Processor loaded\n",
            "\n",
            "üì• Loading model (2-3 min)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc78e47178814af5b6d17eb6a1e8c044",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee3e5eabb6ca40dbabf4094f6d193183",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c1473bff3ab4605974acae725769cc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6baf296197e446929cb4e7b9eae8dd50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23e7a3da8e4c4abb98d2d58b3a4f6826",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5a96fbd0f5a4912990ec97e59d5e989",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.70G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a54168c06b944bcb9bfccdf86ada6184",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "‚úÖ MODEL LOADED!\n",
            "============================================================\n",
            "Device: cuda:0\n",
            "Dtype: torch.bfloat16\n",
            "GPU Memory: 12.69 GB\n",
            "============================================================\n",
            "\n",
            "‚úÖ Ready! Continue to next section for inference.\n"
          ]
        }
      ],
      "source": [
        "# ===== FIXED: Load Model WITHOUT Quantization =====\n",
        "import torch\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from PIL import Image\n",
        "import io, base64, math, warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Login\n",
        "from huggingface_hub import login\n",
        "try:\n",
        "    # Get your token from: https://huggingface.co/settings/tokens\n",
        "    login(token=\"YOUR_HF_TOKEN\")  # ‚Üê Replace with your token!\n",
        "    print(\"‚úÖ Logged in\\n\")\n",
        "except: pass\n",
        "\n",
        "repo = \"Fanbin/ARPO_UITARS1.5_7B\"\n",
        "\n",
        "print(\"ü§ñ Loading model WITHOUT quantization (BFloat16)...\\n\")\n",
        "\n",
        "# Load processor\n",
        "print(\"üì¶ Loading processor...\")\n",
        "processor = AutoProcessor.from_pretrained(repo, trust_remote_code=True)\n",
        "print(\"‚úÖ Processor loaded\\n\")\n",
        "\n",
        "# Load model with AutoModel + trust_remote_code\n",
        "# This automatically loads the custom Qwen2.5-VL code from the repo\n",
        "print(\"üì• Loading model (2-3 min)...\")\n",
        "model = AutoModel.from_pretrained(\n",
        "    repo,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,  # Loads custom model code\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"‚úÖ MODEL LOADED!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Device: {model.device}\")\n",
        "print(f\"Dtype: {model.dtype}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "print(\"‚úÖ Ready! Continue to next section for inference.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Setup Image Processing and Action Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image processing and action space configured!\n"
          ]
        }
      ],
      "source": [
        "# Image processing constants\n",
        "IMAGE_FACTOR = 28\n",
        "MIN_PIXELS = 100 * 28 * 28\n",
        "MAX_PIXELS = 16384 * 28 * 28  # Max 16K tokens\n",
        "\n",
        "def linear_resize(height: int, width: int, factor: int = IMAGE_FACTOR, \n",
        "                  min_pixels: int = MIN_PIXELS, max_pixels: int = MAX_PIXELS):\n",
        "    \"\"\"Resize image maintaining aspect ratio within pixel constraints.\"\"\"\n",
        "    if width * height > max_pixels:\n",
        "        resize_factor = math.sqrt(max_pixels / (width * height))\n",
        "        width, height = int(width * resize_factor), int(height * resize_factor)\n",
        "    if width * height < min_pixels:\n",
        "        resize_factor = math.sqrt(min_pixels / (width * height))\n",
        "        width, height = math.ceil(width * resize_factor), math.ceil(height * resize_factor)\n",
        "    return height, width\n",
        "\n",
        "def preprocess_image(image):\n",
        "    \"\"\"Preprocess image for model input.\"\"\"\n",
        "    if isinstance(image, bytes):\n",
        "        image = Image.open(io.BytesIO(image))\n",
        "    elif isinstance(image, str):\n",
        "        # Assume it's a base64 encoded string or file path\n",
        "        if image.startswith('data:image'):\n",
        "            # Base64 encoded\n",
        "            image_data = base64.b64decode(image.split(',')[1])\n",
        "            image = Image.open(io.BytesIO(image_data))\n",
        "        else:\n",
        "            # File path\n",
        "            image = Image.open(image)\n",
        "    \n",
        "    # Resize if needed\n",
        "    if image.width * image.height > MAX_PIXELS:\n",
        "        resize_factor = math.sqrt(MAX_PIXELS / (image.width * image.height))\n",
        "        new_width = int(image.width * resize_factor)\n",
        "        new_height = int(image.height * resize_factor)\n",
        "        image = image.resize((new_width, new_height))\n",
        "    \n",
        "    if image.width * image.height < MIN_PIXELS:\n",
        "        resize_factor = math.sqrt(MIN_PIXELS / (image.width * image.height))\n",
        "        new_width = math.ceil(image.width * resize_factor)\n",
        "        new_height = math.ceil(image.height * resize_factor)\n",
        "        image = image.resize((new_width, new_height))\n",
        "    \n",
        "    # Convert to RGB if needed\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "    \n",
        "    return image\n",
        "\n",
        "# UITARS Action Space\n",
        "UITARS_ACTION_SPACE = \"\"\"## Action Space\n",
        "The actions you can perform fall into the following categories:\n",
        "\n",
        "- **Mouse Click**: Perform click actions with a bounding box to specify the click target.\n",
        "  - `click(start_box='(x, y)')`: Click at position (x, y)\n",
        "  - `left_double(start_box='(x, y)')`: Double-click at position (x, y)\n",
        "  - `right_single(start_box='(x, y)')`: Right-click at position (x, y)\n",
        "\n",
        "- **Keyboard Input**: Type content or press hotkeys.\n",
        "  - `type(content='text content here')`: Type the given text\n",
        "  - `hotkey(key='key combination')`: Press hotkey (e.g., 'ctrl c', 'ctrl v')\n",
        "  - `press(key='key_name')`: Press a single key\n",
        "\n",
        "- **Scroll**: Scroll in a direction.\n",
        "  - `scroll(start_box='(x, y)', direction='up/down')`: Scroll at position\n",
        "\n",
        "- **Drag**: Drag from one position to another.\n",
        "  - `drag(start_box='(x1, y1)', end_box='(x2, y2)')`: Drag from start to end\n",
        "\n",
        "- **Task Control**:\n",
        "  - `finished()`: Task is complete\n",
        "  - `wait()`: Need to wait for something\n",
        "  - `error_env()`: Environment error\n",
        "\"\"\"\n",
        "\n",
        "print(\"Image processing and action space configured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inference function ready!\n"
          ]
        }
      ],
      "source": [
        "def generate_action(instruction, screenshot, history_images=None, history_responses=None, \n",
        "                   max_new_tokens=4096, temperature=0.0, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Generate action prediction from screenshot and instruction.\n",
        "    \n",
        "    Args:\n",
        "        instruction: Task instruction\n",
        "        screenshot: Current screenshot (PIL Image, bytes, or base64 string)\n",
        "        history_images: List of previous screenshots for multi-turn\n",
        "        history_responses: List of previous model responses\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "        temperature: Sampling temperature\n",
        "        top_p: Top-p sampling parameter\n",
        "    \n",
        "    Returns:\n",
        "        Generated action string\n",
        "    \"\"\"\n",
        "    # Process current screenshot\n",
        "    current_image = preprocess_image(screenshot)\n",
        "    \n",
        "    # Build message history\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful GUI agent assistant.\"}]\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    # Create user prompt\n",
        "    user_prompt = f\"\"\"You are a GUI agent. Your task is to complete the following instruction by interacting with the computer screen.\n",
        "\n",
        "**Instruction**: {instruction}\n",
        "\n",
        "{UITARS_ACTION_SPACE}\n",
        "\n",
        "**Format**: \n",
        "Thought: [Your reasoning about what to do next]\n",
        "Action: [Your action following the action space format]\n",
        "\n",
        "Please provide your response in English.\"\"\"\n",
        "    \n",
        "    # Add history if available\n",
        "    if history_images and history_responses:\n",
        "        history_n = min(15, len(history_images))  # Keep last 15 images\n",
        "        for i in range(len(history_responses)):\n",
        "            if i >= len(history_responses) - history_n:\n",
        "                hist_img = preprocess_image(history_images[i])\n",
        "                messages.append({\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [{\"type\": \"image\", \"image\": hist_img}]\n",
        "                })\n",
        "            messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [{\"type\": \"text\", \"text\": history_responses[i]}]\n",
        "            })\n",
        "    \n",
        "    # Add user prompt and current image\n",
        "    if not history_images or not history_responses:\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": user_prompt}]\n",
        "        })\n",
        "    \n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"type\": \"image\", \"image\": current_image}]\n",
        "    })\n",
        "    \n",
        "    # Process messages\n",
        "    text = processor.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "    \n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    \n",
        "    inputs = inputs.to(model.device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature if temperature > 0 else None,\n",
        "            top_p=top_p if temperature > 0 else None,\n",
        "            do_sample=temperature > 0,\n",
        "        )\n",
        "    \n",
        "    # Trim generated tokens to remove input\n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    \n",
        "    output_text = processor.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )[0]\n",
        "    \n",
        "    return output_text\n",
        "\n",
        "print(\"Inference function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Example Usage: Simple Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Downloading sample Ubuntu desktop screenshot...\n"
          ]
        },
        {
          "ename": "UnidentifiedImageError",
          "evalue": "cannot identify image file <_io.BytesIO object at 0x7faa42619580>",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3850933305.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://upload.wikimedia.org/wikipedia/commons/9/9e/Ubuntu_Desktop_22.04.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mscreenshot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mscreenshot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscreenshot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1920\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1080\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Standardize size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚úÖ Screenshot loaded: {screenshot.size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3578\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3579\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3580\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mUnidentifiedImageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7faa42619580>"
          ]
        }
      ],
      "source": [
        "# Example: Load a desktop screenshot and generate action\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Try to download a sample image, or create synthetic one\n",
        "print(\"üì• Loading desktop screenshot...\")\n",
        "\n",
        "try:\n",
        "    # Try a working direct image URL\n",
        "    url = \"https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/zidane.jpg\"\n",
        "    response = requests.get(url, timeout=10)\n",
        "    response.raise_for_status()\n",
        "    screenshot = Image.open(BytesIO(response.content))\n",
        "    screenshot = screenshot.resize((1920, 1080))\n",
        "    print(f\"‚úÖ Screenshot loaded: {screenshot.size}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Download failed: {e}\")\n",
        "    print(\"üìù Creating synthetic desktop screenshot for demo...\")\n",
        "    # Create a desktop-like image\n",
        "    screenshot = Image.new('RGB', (1920, 1080), color=(45, 45, 48))\n",
        "    draw = ImageDraw.Draw(screenshot)\n",
        "    # Taskbar\n",
        "    draw.rectangle([(0, 1040), (1920, 1080)], fill=(30, 30, 30))\n",
        "    # Window\n",
        "    draw.rectangle([(100, 100), (800, 600)], fill=(255, 255, 255), outline=(150, 150, 150), width=2)\n",
        "    # Dock icons\n",
        "    for i in range(5):\n",
        "        x = 50 + i * 80\n",
        "        draw.rectangle([(x, 1045), (x+60, 1075)], fill=(100, 100, 200))\n",
        "    print(f\"‚úÖ Synthetic screenshot created: {screenshot.size}\")\n",
        "\n",
        "# You can also use your own screenshots:\n",
        "# Option 1: Load from local file\n",
        "# screenshot = Image.open(\"path/to/your/screenshot.png\")\n",
        "\n",
        "# Option 2: Upload file in Colab\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# screenshot = Image.open(list(uploaded.keys())[0])\n",
        "\n",
        "# Define task instruction\n",
        "instruction = \"Click on the Firefox icon to open the browser\"\n",
        "\n",
        "# Generate action\n",
        "print(f\"\\nüìã Instruction: {instruction}\")\n",
        "print(\"üîÆ Generating action...\\n\")\n",
        "\n",
        "action = generate_action(\n",
        "    instruction=instruction,\n",
        "    screenshot=screenshot,\n",
        "    temperature=0.0  # Greedy decoding for deterministic output\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"üéØ GENERATED ACTION:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(action)\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Alternative: Upload Your Own Screenshots\n",
        "\n",
        "If you want to use your own desktop screenshots:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== OPTION 1: Upload file (Colab/Jupyter) =====\n",
        "# Uncomment to upload your own screenshot\n",
        "\"\"\"\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # This will show upload button\n",
        "screenshot = Image.open(list(uploaded.keys())[0])\n",
        "instruction = \"Your task here\"\n",
        "action = generate_action(instruction, screenshot)\n",
        "print(action)\n",
        "\"\"\"\n",
        "\n",
        "# ===== OPTION 2: Load from local file path =====\n",
        "# Uncomment if running locally with a file\n",
        "\"\"\"\n",
        "screenshot = Image.open('/path/to/your/screenshot.png')\n",
        "instruction = \"Your task here\"\n",
        "action = generate_action(instruction, screenshot)\n",
        "print(action)\n",
        "\"\"\"\n",
        "\n",
        "# ===== OPTION 3: Use with OSWorld (if available) =====\n",
        "# Only works if you have OSWorld environment set up locally\n",
        "# NOT compatible with Colab due to Docker/VM requirements\n",
        "\"\"\"\n",
        "import sys\n",
        "sys.path.append('ARPO/OSWorld')\n",
        "from desktop_env.desktop_env import DesktopEnv\n",
        "from io import BytesIO\n",
        "import json\n",
        "\n",
        "# Initialize OSWorld environment (requires Docker/VM)\n",
        "env = DesktopEnv(\n",
        "    action_space=\"pyautogui\",\n",
        "    screen_size=(1920, 1080),\n",
        "    os_type=\"Ubuntu\",\n",
        "    provider_name=\"docker\",\n",
        ")\n",
        "\n",
        "# Load a test task\n",
        "with open('ARPO/OSWorld/evaluation_examples/test_subset32.json', 'r') as f:\n",
        "    tasks = json.load(f)\n",
        "\n",
        "domain = list(tasks.keys())[0]\n",
        "example_id = tasks[domain][0]\n",
        "with open(f'ARPO/OSWorld/evaluation_examples/examples/{domain}/{example_id}.json', 'r') as f:\n",
        "    task = json.load(f)\n",
        "\n",
        "# Get screenshot from environment\n",
        "obs = env.reset(task)\n",
        "screenshot = Image.open(BytesIO(obs['screenshot']))\n",
        "instruction = task['instruction']\n",
        "\n",
        "# Run inference\n",
        "action = generate_action(instruction, screenshot)\n",
        "print(action)\n",
        "\"\"\"\n",
        "\n",
        "print(\"üìù See commented code above for different ways to load screenshots\")\n",
        "print(\"‚úÖ For Colab: Use Option 1 (Upload) or use the sample Ubuntu desktop (Section 5)\")\n",
        "print(\"üñ•Ô∏è  For local with OSWorld: Use Option 3\")\n",
        "\n",
        "\n",
        "    def __init__(self, model, processor, max_history=15):\n",
        "        self.model = model\n",
        "        self.processor = processor\n",
        "        self.max_history = max_history\n",
        "        self.history_images = []\n",
        "        self.history_responses = []\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset history for new task.\"\"\"\n",
        "        self.history_images = []\n",
        "        self.history_responses = []\n",
        "    \n",
        "    def predict(self, instruction, screenshot, temperature=0.0, top_p=0.9, max_new_tokens=4096):\n",
        "        \"\"\"Predict next action.\"\"\"\n",
        "        # Generate action\n",
        "        action = generate_action(\n",
        "            instruction=instruction,\n",
        "            screenshot=screenshot,\n",
        "            history_images=self.history_images,\n",
        "            history_responses=self.history_responses,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            max_new_tokens=max_new_tokens\n",
        "        )\n",
        "        \n",
        "        # Update history\n",
        "        self.history_images.append(screenshot)\n",
        "        self.history_responses.append(action)\n",
        "        \n",
        "        # Keep only last max_history items\n",
        "        if len(self.history_images) > self.max_history:\n",
        "            self.history_images = self.history_images[-self.max_history:]\n",
        "            self.history_responses = self.history_responses[-self.max_history:]\n",
        "        \n",
        "        return action\n",
        "    \n",
        "    def is_finished(self, action):\n",
        "        \"\"\"Check if task is finished.\"\"\"\n",
        "        return 'finished()' in action.lower()\n",
        "\n",
        "# Create agent\n",
        "agent = UITARSInferenceAgent(model, processor)\n",
        "\n",
        "print(\"Multi-turn agent ready!\")\n",
        "print(\"\\nExample usage:\")\n",
        "print(\"agent.reset()  # Start new task\")\n",
        "print(\"action = agent.predict(instruction, screenshot)\")\n",
        "print(\"if agent.is_finished(action): # Task complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Multi-Turn Interaction Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-turn interaction example\n",
        "class UITARSInferenceAgent:\n",
        "    def __init__(self, model, processor, max_history=15):\n",
        "        self.model = model\n",
        "        self.processor = processor\n",
        "        self.max_history = max_history\n",
        "        self.history_images = []\n",
        "        self.history_responses = []\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset history for new task.\"\"\"\n",
        "        self.history_images = []\n",
        "        self.history_responses = []\n",
        "    \n",
        "    def predict(self, instruction, screenshot, temperature=0.0, top_p=0.9, max_new_tokens=4096):\n",
        "        \"\"\"Predict next action.\"\"\"\n",
        "        # Generate action\n",
        "        action = generate_action(\n",
        "            instruction=instruction,\n",
        "            screenshot=screenshot,\n",
        "            history_images=self.history_images,\n",
        "            history_responses=self.history_responses,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            max_new_tokens=max_new_tokens\n",
        "        )\n",
        "        \n",
        "        # Update history\n",
        "        self.history_images.append(screenshot)\n",
        "        self.history_responses.append(action)\n",
        "        \n",
        "        # Keep only last max_history items\n",
        "        if len(self.history_images) > self.max_history:\n",
        "            self.history_images = self.history_images[-self.max_history:]\n",
        "            self.history_responses = self.history_responses[-self.max_history:]\n",
        "        \n",
        "        return action\n",
        "    \n",
        "    def is_finished(self, action):\n",
        "        \"\"\"Check if task is finished.\"\"\"\n",
        "        return 'finished()' in action.lower()\n",
        "\n",
        "# Create agent\n",
        "agent = UITARSInferenceAgent(model, processor)\n",
        "\n",
        "print(\"Multi-turn agent ready!\")\n",
        "print(\"\\nExample usage:\")\n",
        "print(\"agent.reset()  # Start new task\")\n",
        "print(\"action = agent.predict(instruction, screenshot)\")\n",
        "print(\"if agent.is_finished(action): # Task complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Action Parsing Utilities\n",
        "\n",
        "These utilities help parse model outputs into executable actions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thought: I need to click on the Firefox icon to open the browser.\n",
            "Action: click(start_box='(100, 200)')\n",
            "Parsed: {'function': 'click', 'args': {'start_box': '(100, 200)'}}\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import ast\n",
        "\n",
        "def parse_action(action_str):\n",
        "    \"\"\"Parse action string into structured format.\"\"\"\n",
        "    try:\n",
        "        node = ast.parse(action_str.strip(), mode='eval')\n",
        "        if not isinstance(node, ast.Expression):\n",
        "            return None\n",
        "        \n",
        "        call = node.body\n",
        "        if not isinstance(call, ast.Call):\n",
        "            return None\n",
        "        \n",
        "        # Get function name\n",
        "        if isinstance(call.func, ast.Name):\n",
        "            func_name = call.func.id\n",
        "        elif isinstance(call.func, ast.Attribute):\n",
        "            func_name = call.func.attr\n",
        "        else:\n",
        "            return None\n",
        "        \n",
        "        # Get keyword arguments\n",
        "        kwargs = {}\n",
        "        for kw in call.keywords:\n",
        "            key = kw.arg\n",
        "            if isinstance(kw.value, ast.Constant):\n",
        "                value = kw.value.value\n",
        "            elif isinstance(kw.value, ast.Str):\n",
        "                value = kw.value.s\n",
        "            else:\n",
        "                value = None\n",
        "            kwargs[key] = value\n",
        "        \n",
        "        return {\n",
        "            'function': func_name,\n",
        "            'args': kwargs\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to parse action '{action_str}': {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_thought_and_action(response):\n",
        "    \"\"\"Extract thought and action from model response.\"\"\"\n",
        "    thought = None\n",
        "    action = None\n",
        "    \n",
        "    # Extract thought\n",
        "    thought_match = re.search(r\"Thought:\\s*(.+?)(?=\\s*Action:|$)\", response, re.DOTALL)\n",
        "    if thought_match:\n",
        "        thought = thought_match.group(1).strip()\n",
        "    \n",
        "    # Extract action\n",
        "    if \"Action:\" in response:\n",
        "        action = response.split(\"Action:\")[-1].strip()\n",
        "    \n",
        "    return thought, action\n",
        "\n",
        "# Test parsing\n",
        "test_response = \"\"\"Thought: I need to click on the Firefox icon to open the browser.\n",
        "Action: click(start_box='(100, 200)')\"\"\"\n",
        "\n",
        "thought, action = extract_thought_and_action(test_response)\n",
        "print(f\"Thought: {thought}\")\n",
        "print(f\"Action: {action}\")\n",
        "\n",
        "if action:\n",
        "    parsed = parse_action(action)\n",
        "    print(f\"Parsed: {parsed}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Integration with OSWorld Environment (Local Only)\n",
        "\n",
        "**Note**: This section only works on local machines with Docker/VM, not on Colab.\n",
        "\n",
        "To use this model with OSWorld environment locally:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "OSWorld integration example (commented out)\n",
            "Uncomment and modify based on your OSWorld setup.\n"
          ]
        }
      ],
      "source": [
        "# Example integration with OSWorld (requires OSWorld setup)\n",
        "# Uncomment and modify based on your OSWorld setup\n",
        "\n",
        "\"\"\"\n",
        "import sys\n",
        "sys.path.append('OSWorld')\n",
        "\n",
        "from desktop_env.desktop_env import DesktopEnv\n",
        "import json\n",
        "\n",
        "# Initialize environment\n",
        "env = DesktopEnv(\n",
        "    path_to_vm=None,  # Set your VM path\n",
        "    action_space=\"pyautogui\",\n",
        "    screen_size=(1920, 1080),\n",
        "    headless=False,\n",
        "    os_type=\"Ubuntu\",\n",
        "    provider_name=\"docker\",\n",
        "    require_a11y_tree=False,\n",
        ")\n",
        "\n",
        "# Load a task\n",
        "with open('OSWorld/evaluation_examples/test_subset32.json', 'r') as f:\n",
        "    test_tasks = json.load(f)\n",
        "\n",
        "# Reset agent\n",
        "agent.reset()\n",
        "\n",
        "# Get first task\n",
        "domain = list(test_tasks.keys())[0]\n",
        "example_id = test_tasks[domain][0]\n",
        "config_file = f'OSWorld/evaluation_examples/examples/{domain}/{example_id}.json'\n",
        "\n",
        "with open(config_file, 'r') as f:\n",
        "    example = json.load(f)\n",
        "\n",
        "instruction = example['instruction']\n",
        "print(f\"Task: {instruction}\")\n",
        "\n",
        "# Reset environment\n",
        "obs = env.reset(example)\n",
        "\n",
        "# Run for max 15 steps\n",
        "max_steps = 15\n",
        "for step in range(max_steps):\n",
        "    print(f\"\\nStep {step + 1}/{max_steps}\")\n",
        "    \n",
        "    # Get screenshot\n",
        "    screenshot = obs['screenshot']\n",
        "    \n",
        "    # Predict action\n",
        "    action_text = agent.predict(instruction, screenshot)\n",
        "    print(f\"Action: {action_text}\")\n",
        "    \n",
        "    # Check if finished\n",
        "    if agent.is_finished(action_text):\n",
        "        print(\"Task completed!\")\n",
        "        break\n",
        "    \n",
        "    # Parse and execute action (requires action parsing from OSWorld/mm_agents/uitars_agent.py)\n",
        "    # obs, reward, done, info = env.step(parsed_action)\n",
        "\n",
        "env.close()\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nOSWorld integration example (commented out)\")\n",
        "print(\"Uncomment and modify based on your OSWorld setup.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Memory and Performance Tips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def print_memory_usage():\n",
        "    \"\"\"Print current GPU memory usage.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "        print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "    else:\n",
        "        print(\"CUDA not available\")\n",
        "\n",
        "print_memory_usage()\n",
        "\n",
        "print(\"\\n=== Memory Optimization Tips ===\")\n",
        "print(\"1. Using 4-bit quantization reduces memory by ~4x\")\n",
        "print(\"2. Reduce max_pixels to 8192*28*28 or 4096*28*28 for lower memory\")\n",
        "print(\"3. Reduce max_new_tokens to 2048 or 1024\")\n",
        "print(\"4. Use torch.cuda.empty_cache() between inferences if needed\")\n",
        "print(\"5. Limit history_n to 5-10 instead of 15 for less memory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for reproducibility\n",
        "config = {\n",
        "    \"model_name\": \"Fanbin/ARPO_UITARS1.5_7B\",\n",
        "    \"quantization\": \"4-bit (nf4)\",\n",
        "    \"compute_dtype\": \"float16\",\n",
        "    \"max_pixels\": MAX_PIXELS,\n",
        "    \"min_pixels\": MIN_PIXELS,\n",
        "    \"max_history\": 15,\n",
        "    \"max_new_tokens\": 4096,\n",
        "    \"temperature\": 0.0,\n",
        "    \"top_p\": 0.9,\n",
        "}\n",
        "\n",
        "import json\n",
        "print(\"Configuration:\")\n",
        "print(json.dumps(config, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. ‚úÖ Loading ARPO UITARS model with 4-bit quantization (works on Colab!)\n",
        "2. ‚úÖ Preprocessing screenshots for optimal performance\n",
        "3. ‚úÖ Single-turn and multi-turn inference\n",
        "4. ‚úÖ Using real desktop screenshots (Ubuntu desktop sample)\n",
        "5. ‚úÖ Multiple ways to load screenshots (upload, file, URL)\n",
        "6. ‚úÖ Action parsing utilities\n",
        "7. ‚úÖ OSWorld integration (for local machines)\n",
        "\n",
        "**This notebook is ready to run on:**\n",
        "- ‚úÖ Google Colab (Free T4 GPU)\n",
        "- ‚úÖ Local Jupyter\n",
        "- ‚úÖ Kaggle Notebooks\n",
        "- ‚úÖ Any environment with 8GB+ GPU\n",
        "\n",
        "**Next Steps:**\n",
        "- Upload your own desktop screenshots to test\n",
        "- Try different instructions\n",
        "- Set up OSWorld environment for full evaluation\n",
        "- Implement action execution pipeline\n",
        "- Run evaluation on test tasks\n",
        "\n",
        "**Citation:**\n",
        "```bibtex\n",
        "@article{lu2025arpo,\n",
        "  title={ARPO: End-to-End Policy Optimization for GUI Agents with Experience Replay},\n",
        "  author={Fanbin Lu and Zhisheng Zhong and Shu Liu and Chi-Wing Fu and Jiaya Jia},\n",
        "  journal={arxiv},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "**Model Link:** [Fanbin/ARPO_UITARS1.5_7B](https://huggingface.co/Fanbin/ARPO_UITARS1.5_7B)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
