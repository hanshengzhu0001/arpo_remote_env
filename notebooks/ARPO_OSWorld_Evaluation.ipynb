{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ARPO OSWorld Evaluation - GPU Testing\n",
        "\n",
        "This notebook runs the **complete OSWorld evaluation** on 10 tasks (5 original + 5 noisy) using the GPU inference server.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **GPU Server Running**: Start `notebooks/GPU_Server_for_OSWorld.ipynb` on Colab first\n",
        "2. **VMware VM Ready**: Your Mac OSWorld VM should be set up and ready\n",
        "3. **Server URL**: Copy the ngrok URL from the GPU server notebook\n",
        "\n",
        "---\n",
        "\n",
        "## Setup Summary\n",
        "\n",
        "- **Model**: ARPO UITARS 7B (running on Colab GPU)\n",
        "- **Tasks**: 10 OSWorld tasks (Chrome domain)\n",
        "- **Expected Time**: ~10-15 minutes (vs 10 hours on CPU!)\n",
        "- **Results**: Saved to `results/gpu_eval/`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Check Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Get project root\n",
        "ARPO_ROOT = Path(\"/Users/hanszhu/Desktop/ARPO_replicate\")\n",
        "os.chdir(ARPO_ROOT)\n",
        "\n",
        "# Add OSWorld to path\n",
        "sys.path.insert(0, str(ARPO_ROOT / \"OSWorld\"))\n",
        "\n",
        "print(f\"‚úÖ Working directory: {os.getcwd()}\")\n",
        "print(f\"‚úÖ OSWorld path: {ARPO_ROOT / 'OSWorld'}\")\n",
        "print(f\"‚úÖ Test data: {ARPO_ROOT / 'test_data' / 'osworld_examples'}\")\n",
        "print(f\"‚úÖ Results will be saved to: {ARPO_ROOT / 'results' / 'gpu_eval'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure GPU Server URL\n",
        "\n",
        "**‚ö†Ô∏è Important**: Update this with your actual ngrok URL from the GPU server notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# UPDATE THIS WITH YOUR NGROK URL FROM COLAB!\n",
        "GPU_SERVER_URL = \"https://YOUR-NGROK-URL.ngrok.io\"  # Example: https://1234-56-78-90-12.ngrok.io\n",
        "\n",
        "# Test connection\n",
        "import requests\n",
        "\n",
        "if GPU_SERVER_URL == \"https://YOUR-NGROK-URL.ngrok.io\":\n",
        "    print(\"‚ö†Ô∏è  WARNING: You need to update GPU_SERVER_URL with your actual ngrok URL!\")\n",
        "    print(\"   Get it from the GPU server notebook (Cell 4 output)\")\n",
        "else:\n",
        "    try:\n",
        "        response = requests.get(f\"{GPU_SERVER_URL}/health\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"‚úÖ GPU Server is reachable: {GPU_SERVER_URL}\")\n",
        "            print(f\"‚úÖ Server status: {response.json()}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Server returned status {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Cannot reach server: {e}\")\n",
        "        print(f\"   Make sure GPU server notebook is running on Colab!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Update OSWorld Agent Configuration\n",
        "\n",
        "Update the agent to use the Colab GPU server instead of localhost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fileinput\n",
        "import shutil\n",
        "\n",
        "# Backup original\n",
        "agent_file = ARPO_ROOT / \"OSWorld\" / \"mm_agents\" / \"uitars_agent.py\"\n",
        "backup_file = agent_file.with_suffix('.py.backup')\n",
        "\n",
        "if not backup_file.exists():\n",
        "    shutil.copy(agent_file, backup_file)\n",
        "    print(f\"‚úÖ Created backup: {backup_file}\")\n",
        "\n",
        "# Update base_url\n",
        "agent_content = agent_file.read_text()\n",
        "if \"localhost:9000\" in agent_content:\n",
        "    updated_content = agent_content.replace(\n",
        "        'base_url=\"http://localhost:9000/v1\"',\n",
        "        f'base_url=\"{GPU_SERVER_URL}/v1\"'\n",
        "    )\n",
        "    agent_file.write_text(updated_content)\n",
        "    print(f\"‚úÖ Updated agent to use: {GPU_SERVER_URL}/v1\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Agent already configured (not using localhost)\")\n",
        "    print(f\"   Current config will be used\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run Evaluation on Original Tasks (5 tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. View Results for Original Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def analyze_results(result_dir):\n",
        "    \"\"\"Analyze OSWorld evaluation results\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    # Find all result.txt files\n",
        "    for result_file in Path(result_dir).rglob(\"result.txt\"):\n",
        "        task_id = result_file.parent.name\n",
        "        domain = result_file.parent.parent.name\n",
        "        \n",
        "        try:\n",
        "            score = float(result_file.read_text().strip())\n",
        "            results.append({\n",
        "                \"task_id\": task_id,\n",
        "                \"domain\": domain,\n",
        "                \"score\": score\n",
        "            })\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    if not results:\n",
        "        print(\"‚ö†Ô∏è  No results found yet\")\n",
        "        return None\n",
        "    \n",
        "    # Print summary\n",
        "    print(\"=\"*70)\n",
        "    print(f\"üìä Results Summary ({len(results)} tasks)\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for r in results:\n",
        "        status = \"‚úÖ PASS\" if r[\"score\"] >= 0.9 else \"‚ùå FAIL\"\n",
        "        print(f\"{status} | {r['task_id'][:20]:20s} | Score: {r['score']:.2f}\")\n",
        "    \n",
        "    avg_score = sum(r[\"score\"] for r in results) / len(results)\n",
        "    success_rate = sum(1 for r in results if r[\"score\"] >= 0.9) / len(results) * 100\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(f\"Average Score: {avg_score:.3f}\")\n",
        "    print(f\"Success Rate:  {success_rate:.1f}% ({sum(1 for r in results if r['score'] >= 0.9)}/{len(results)})\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Analyze original tasks\n",
        "print(\"\\nüîç Analyzing ORIGINAL task results...\\n\")\n",
        "original_results = analyze_results(results_dir_original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "osworld testing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Create results directory\n",
        "results_dir_noisy = ARPO_ROOT / \"results\" / \"gpu_eval_noisy\"\n",
        "results_dir_noisy.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"üöÄ Starting evaluation on 5 NOISY tasks...\")\n",
        "print(f\"üìÅ Results will be saved to: {results_dir_noisy}\")\n",
        "print(f\"‚è±Ô∏è  Expected time: ~5-8 minutes\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Run OSWorld evaluation on noisy tasks\n",
        "cmd = [\n",
        "    \"python\", \"run_uitars.py\",\n",
        "    \"--headless\",\n",
        "    \"--observation_type\", \"screenshot\",\n",
        "    \"--max_steps\", \"15\",\n",
        "    \"--model\", \"arpo-uitars-7b\",\n",
        "    \"--temperature\", \"0.6\",\n",
        "    \"--max_tokens\", \"256\",\n",
        "    \"--test_all_meta_path\", \"../test_data/osworld_examples/test_10tasks_noisy.json\",\n",
        "    \"--result_dir\", str(results_dir_noisy),\n",
        "]\n",
        "\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        cmd,\n",
        "        cwd=ARPO_ROOT / \"OSWorld\",\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=1800  # 30 min timeout\n",
        "    )\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"‚úÖ Evaluation complete in {elapsed/60:.1f} minutes!\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Show last 50 lines\n",
        "    print(\"\\nüìä Last 50 lines of output:\")\n",
        "    print(\"\\n\".join(result.stdout.split(\"\\n\")[-50:]))\n",
        "    \n",
        "    if result.returncode != 0:\n",
        "        print(f\"\\n‚ö†Ô∏è  Warning: Process returned code {result.returncode}\")\n",
        "        print(\"Last 20 lines of stderr:\")\n",
        "        print(\"\\n\".join(result.stderr.split(\"\\n\")[-20:]))\n",
        "        \n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"‚ùå Evaluation timed out after 30 minutes\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. View Results for Noisy Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "print(\"\\nüîç Analyzing NOISY task results...\\n\")\n",
        "noisy_results = analyze_results(results_dir_noisy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if original_results and noisy_results:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä ORIGINAL vs NOISY Comparison\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    orig_avg = sum(r[\"score\"] for r in original_results) / len(original_results)\n",
        "    noisy_avg = sum(r[\"score\"] for r in noisy_results) / len(noisy_results)\n",
        "    \n",
        "    orig_success = sum(1 for r in original_results if r[\"score\"] >= 0.9) / len(original_results) * 100\n",
        "    noisy_success = sum(1 for r in noisy_results if r[\"score\"] >= 0.9) / len(noisy_results) * 100\n",
        "    \n",
        "    print(f\"\\nOriginal Tasks ({len(original_results)}):\")\n",
        "    print(f\"  Average Score:  {orig_avg:.3f}\")\n",
        "    print(f\"  Success Rate:   {orig_success:.1f}%\")\n",
        "    \n",
        "    print(f\"\\nNoisy Tasks ({len(noisy_results)}):\")\n",
        "    print(f\"  Average Score:  {noisy_avg:.3f}\")\n",
        "    print(f\"  Success Rate:   {noisy_success:.1f}%\")\n",
        "    \n",
        "    print(f\"\\nRobustness (Noisy/Original):\")\n",
        "    print(f\"  Score Ratio:    {noisy_avg/orig_avg:.2%}\")\n",
        "    print(f\"  Success Ratio:  {noisy_success/orig_success:.2%}\" if orig_success > 0 else \"  Success Ratio:  N/A\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Expected performance (from ARPO paper)\n",
        "    print(\"\\nüìö Expected Performance (from ARPO paper):\")\n",
        "    print(\"  ARPO UITARS1.5 7B on OSWorld: ~22.6% success rate\")\n",
        "    print(\"  (This is a subset of 10 tasks, so results may vary)\")\n",
        "    print(\"=\"*70)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Run cells 4 and 6 first to get results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Inspect Individual Task Trajectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def view_trajectory(result_dir, task_id):\n",
        "    \"\"\"View detailed trajectory for a specific task\"\"\"\n",
        "    traj_file = None\n",
        "    for f in Path(result_dir).rglob(f\"{task_id}/traj.jsonl\"):\n",
        "        traj_file = f\n",
        "        break\n",
        "    \n",
        "    if not traj_file:\n",
        "        print(f\"‚ö†Ô∏è  Trajectory not found for {task_id}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\nüìù Trajectory: {task_id}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    with open(traj_file) as f:\n",
        "        steps = [json.loads(line) for line in f]\n",
        "    \n",
        "    for i, step in enumerate(steps, 1):\n",
        "        print(f\"\\nStep {i}:\")\n",
        "        if \"prediction\" in step:\n",
        "            pred = step[\"prediction\"]\n",
        "            if isinstance(pred, str):\n",
        "                # Show first 200 chars\n",
        "                print(f\"  Prediction: {pred[:200]}...\")\n",
        "            else:\n",
        "                print(f\"  Prediction: {pred}\")\n",
        "        if \"action\" in step:\n",
        "            print(f\"  Action: {step['action']}\")\n",
        "        if \"reward\" in step:\n",
        "            print(f\"  Reward: {step['reward']}\")\n",
        "        if \"done\" in step:\n",
        "            print(f\"  Done: {step['done']}\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "\n",
        "# Example: View first task from original results\n",
        "if original_results:\n",
        "    first_task = original_results[0][\"task_id\"]\n",
        "    print(f\"\\nüîç Viewing trajectory for first task: {first_task}\")\n",
        "    view_trajectory(results_dir_original, first_task)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No results available yet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Restore Original Agent Configuration\n",
        "\n",
        "After evaluation, restore the agent to use localhost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Restore backup\n",
        "if backup_file.exists():\n",
        "    shutil.copy(backup_file, agent_file)\n",
        "    print(f\"‚úÖ Restored original agent configuration\")\n",
        "    print(f\"   Agent is now using localhost:9000 again\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No backup found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What This Notebook Does\n",
        "\n",
        "1. ‚úÖ Connects to Colab GPU server (via ngrok)\n",
        "2. ‚úÖ Runs 5 original OSWorld tasks\n",
        "3. ‚úÖ Runs 5 noisy OSWorld tasks\n",
        "4. ‚úÖ Analyzes results and computes success rates\n",
        "5. ‚úÖ Compares robustness (original vs noisy)\n",
        "6. ‚úÖ Restores original configuration\n",
        "\n",
        "### Results Location\n",
        "\n",
        "- **Original**: `results/gpu_eval_original/`\n",
        "- **Noisy**: `results/gpu_eval_noisy/`\n",
        "\n",
        "Each task folder contains:\n",
        "- `traj.jsonl` - Step-by-step log\n",
        "- `result.txt` - Final score (0.0 or 1.0)\n",
        "- `step_*.png` - Screenshots\n",
        "- `recording.mp4` - Video\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Analyze failed tasks to understand errors\n",
        "- Compare with ARPO paper results (~22.6% on full OSWorld)\n",
        "- Use insights for further training/fine-tuning"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
