{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ARPO OSWorld Evaluation - Chrome Tasks (Original vs Noisy)\n",
        "\n",
        "This notebook evaluates **10 Chrome tasks** (original + noisy versions) to test model robustness.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **GPU Server Running**: Start `notebooks/GPU_Server_for_OSWorld.ipynb` on Colab first\n",
        "2. **VMware VM Ready**: Your Mac OSWorld VM should be set up and ready\n",
        "3. **Server URL**: Copy the ngrok URL from the GPU server notebook\n",
        "\n",
        "---\n",
        "\n",
        "## Setup Summary\n",
        "\n",
        "- **Model**: ARPO UITARS 7B (running on Colab GPU)\n",
        "- **Tasks**: 10 Chrome tasks (original) + 10 Chrome tasks (noisy)\n",
        "- **Total**: 20 tasks\n",
        "- **Expected Time**: ~1.5 hours (20 tasks √ó ~4.5 min per task)\n",
        "- **Results**: Saved to `results/gpu_eval_chrome_10/` and `results/gpu_eval_chrome_noisy_10/`\n",
        "- **Dataset**: 128 total tasks (18 Chrome, 19 VS Code, 19 GIMP, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Check Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Working directory: /Users/hanszhu/Desktop/ARPO_replicate\n",
            "‚úÖ OSWorld path: /Users/hanszhu/Desktop/ARPO_replicate/OSWorld\n",
            "‚úÖ Test data: /Users/hanszhu/Desktop/ARPO_replicate/test_data/osworld_examples\n",
            "‚úÖ Results will be saved to: /Users/hanszhu/Desktop/ARPO_replicate/results/gpu_eval\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Get project root\n",
        "ARPO_ROOT = Path(\"/Users/hanszhu/Desktop/ARPO_replicate\")\n",
        "os.chdir(ARPO_ROOT)\n",
        "\n",
        "# Add OSWorld to path\n",
        "sys.path.insert(0, str(ARPO_ROOT / \"OSWorld\"))\n",
        "\n",
        "print(f\"‚úÖ Working directory: {os.getcwd()}\")\n",
        "print(f\"‚úÖ OSWorld path: {ARPO_ROOT / 'OSWorld'}\")\n",
        "print(f\"‚úÖ Test data: {ARPO_ROOT / 'test_data' / 'osworld_examples'}\")\n",
        "print(f\"‚úÖ Results will be saved to: {ARPO_ROOT / 'results' / 'gpu_eval'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure GPU Server URL\n",
        "\n",
        "**‚ö†Ô∏è Important**: Update this with your actual ngrok URL from the GPU server notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPU Server is reachable: https://miller-unshapeable-melany.ngrok-free.dev\n",
            "‚úÖ Server status: {'model': 'arpo-uitars-7b', 'status': 'healthy'}\n"
          ]
        }
      ],
      "source": [
        "# UPDATE THIS WITH YOUR NGROK URL FROM COLAB!\n",
        "GPU_SERVER_URL = \"https://miller-unshapeable-melany.ngrok-free.dev\"  # Example: https://1234-56-78-90-12.ngrok.io\n",
        "\n",
        "# Test connection\n",
        "import requests\n",
        "\n",
        "if GPU_SERVER_URL == \"https://YOUR-NGROK-URL.ngrok.io\":\n",
        "    print(\"‚ö†Ô∏è  WARNING: You need to update GPU_SERVER_URL with your actual ngrok URL!\")\n",
        "    print(\"   Get it from the GPU server notebook (Cell 4 output)\")\n",
        "else:\n",
        "    try:\n",
        "        response = requests.get(f\"{GPU_SERVER_URL}/health\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"‚úÖ GPU Server is reachable: {GPU_SERVER_URL}\")\n",
        "            print(f\"‚úÖ Server status: {response.json()}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Server returned status {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Cannot reach server: {e}\")\n",
        "        print(f\"   Make sure GPU server notebook is running on Colab!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Update OSWorld Agent Configuration\n",
        "\n",
        "Update the agent to use the Colab GPU server instead of localhost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  Agent already configured (not using localhost)\n",
            "   Current config will be used\n"
          ]
        }
      ],
      "source": [
        "import fileinput\n",
        "import shutil\n",
        "\n",
        "# Backup original\n",
        "agent_file = ARPO_ROOT / \"OSWorld\" / \"mm_agents\" / \"uitars_agent.py\"\n",
        "backup_file = agent_file.with_suffix('.py.backup')\n",
        "\n",
        "if not backup_file.exists():\n",
        "    shutil.copy(agent_file, backup_file)\n",
        "    print(f\"‚úÖ Created backup: {backup_file}\")\n",
        "\n",
        "# Update base_url\n",
        "agent_content = agent_file.read_text()\n",
        "if \"localhost:9000\" in agent_content:\n",
        "    updated_content = agent_content.replace(\n",
        "        'base_url=\"http://localhost:9001/v1\"',\n",
        "        f'base_url=\"{GPU_SERVER_URL}/v1\"'\n",
        "    )\n",
        "    agent_file.write_text(updated_content)\n",
        "    print(f\"‚úÖ Updated agent to use: {GPU_SERVER_URL}/v1\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Agent already configured (not using localhost)\")\n",
        "    print(f\"   Current config will be used\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run Evaluation on Original Chrome Tasks (10 tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßπ Clearing previous results from: /Users/hanszhu/Desktop/ARPO_replicate/results/gpu_eval_chrome_10\n",
            "üöÄ Starting evaluation on 10 CHROME tasks (original)...\n",
            "üìÅ Results will be saved to: /Users/hanszhu/Desktop/ARPO_replicate/results/gpu_eval_chrome_10\n",
            "‚è±Ô∏è  Expected time: ~45 minutes (10 tasks √ó ~4.5 min)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "# Create/clear results directory\n",
        "results_dir_chrome = ARPO_ROOT / \"results\" / \"gpu_eval_chrome_10\"\n",
        "\n",
        "# Clear previous results if they exist\n",
        "if results_dir_chrome.exists():\n",
        "    print(f\"üßπ Clearing previous results from: {results_dir_chrome}\")\n",
        "    shutil.rmtree(results_dir_chrome)\n",
        "    \n",
        "results_dir_chrome.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"üöÄ Starting evaluation on 10 CHROME tasks (original)...\")\n",
        "print(f\"üìÅ Results will be saved to: {results_dir_chrome}\")\n",
        "print(f\"‚è±Ô∏è  Expected time: ~45 minutes (10 tasks √ó ~4.5 min)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Run OSWorld evaluation\n",
        "cmd = [\n",
        "    \"python\", \"run_uitars.py\",\n",
        "    \"--headless\",\n",
        "    \"--observation_type\", \"screenshot\",\n",
        "    \"--max_steps\", \"15\",\n",
        "    \"--model\", \"arpo-uitars-7b\",\n",
        "    \"--temperature\", \"0.6\",\n",
        "    \"--max_tokens\", \"256\",\n",
        "    \"--test_config_base_dir\", \"../test_data/osworld_examples\",\n",
        "    \"--test_all_meta_path\", \"../test_data/osworld_examples/test_chrome_10.json\",\n",
        "    \"--result_dir\", str(results_dir_chrome),\n",
        "]\n",
        "\n",
        "try:\n",
        "    # Run from OSWorld directory\n",
        "    result = subprocess.run(\n",
        "        cmd,\n",
        "        cwd=ARPO_ROOT / \"OSWorld\",\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=3600  # 60 min timeout (10 tasks √ó ~4.5 min + buffer)\n",
        "    )\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"‚úÖ Evaluation complete in {elapsed/60:.1f} minutes!\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Show last 50 lines of output\n",
        "    print(\"\\nüìä Last 50 lines of output:\")\n",
        "    print(\"\\n\".join(result.stdout.split(\"\\n\")[-50:]))\n",
        "    \n",
        "    if result.returncode != 0:\n",
        "        print(f\"\\n‚ö†Ô∏è  Warning: Process returned code {result.returncode}\")\n",
        "        print(\"Last 20 lines of stderr:\")\n",
        "        print(\"\\n\".join(result.stderr.split(\"\\n\")[-20:]))\n",
        "        \n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"‚ùå Evaluation timed out after 60 minutes\")\n",
        "    print(\"‚ö†Ô∏è  Some tasks may have completed - check results folder\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. View Results for Original Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Analyzing ORIGINAL task results...\n",
            "\n",
            "======================================================================\n",
            "üìä Results Summary (5 tasks)\n",
            "======================================================================\n",
            "‚ùå FAIL | fc6d8143-9452-4171-9 | Score: 0.00\n",
            "‚ùå FAIL | 44ee5668-ecd5-4366-a | Score: 0.00\n",
            "‚ùå FAIL | f79439ad-3ee8-4f99-a | Score: 0.00\n",
            "‚ùå FAIL | f5d96daf-83a8-4c86-9 | Score: 0.00\n",
            "‚úÖ PASS | f3b19d1e-2d48-44e9-b | Score: 1.00\n",
            "======================================================================\n",
            "Average Score: 0.200\n",
            "Success Rate:  20.0% (1/5)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def analyze_results(result_dir):\n",
        "    \"\"\"Analyze OSWorld evaluation results\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    # Find all result.txt files\n",
        "    for result_file in Path(result_dir).rglob(\"result.txt\"):\n",
        "        task_id = result_file.parent.name\n",
        "        domain = result_file.parent.parent.name\n",
        "        \n",
        "        try:\n",
        "            score = float(result_file.read_text().strip())\n",
        "            results.append({\n",
        "                \"task_id\": task_id,\n",
        "                \"domain\": domain,\n",
        "                \"score\": score\n",
        "            })\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    if not results:\n",
        "        print(\"‚ö†Ô∏è  No results found yet\")\n",
        "        return None\n",
        "    \n",
        "    # Print summary\n",
        "    print(\"=\"*70)\n",
        "    print(f\"üìä Results Summary ({len(results)} tasks)\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for r in results:\n",
        "        status = \"‚úÖ PASS\" if r[\"score\"] >= 0.9 else \"‚ùå FAIL\"\n",
        "        print(f\"{status} | {r['task_id'][:20]:20s} | Score: {r['score']:.2f}\")\n",
        "    \n",
        "    avg_score = sum(r[\"score\"] for r in results) / len(results)\n",
        "    success_rate = sum(1 for r in results if r[\"score\"] >= 0.9) / len(results) * 100\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(f\"Average Score: {avg_score:.3f}\")\n",
        "    print(f\"Success Rate:  {success_rate:.1f}% ({sum(1 for r in results if r['score'] >= 0.9)}/{len(results)})\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Analyze Chrome tasks\n",
        "print(\"\\nüîç Analyzing CHROME task results...\\n\")\n",
        "chrome_results = analyze_results(results_dir_chrome)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "osworld testing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "# Create/clear results directory\n",
        "results_dir_chrome_noisy = ARPO_ROOT / \"results\" / \"gpu_eval_chrome_noisy_10\"\n",
        "\n",
        "# Clear previous results if they exist\n",
        "if results_dir_chrome_noisy.exists():\n",
        "    print(f\"üßπ Clearing previous results from: {results_dir_chrome_noisy}\")\n",
        "    shutil.rmtree(results_dir_chrome_noisy)\n",
        "    \n",
        "results_dir_chrome_noisy.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"üöÄ Starting evaluation on 10 CHROME tasks (noisy)...\")\n",
        "print(f\"üìÅ Results will be saved to: {results_dir_chrome_noisy}\")\n",
        "print(f\"‚è±Ô∏è  Expected time: ~45 minutes (10 tasks √ó ~4.5 min)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Run OSWorld evaluation on noisy Chrome tasks\n",
        "cmd = [\n",
        "    \"python\", \"run_uitars.py\",\n",
        "    \"--headless\",\n",
        "    \"--observation_type\", \"screenshot\",\n",
        "    \"--max_steps\", \"15\",\n",
        "    \"--model\", \"arpo-uitars-7b\",\n",
        "    \"--temperature\", \"0.6\",\n",
        "    \"--max_tokens\", \"256\",\n",
        "    \"--test_config_base_dir\", \"../test_data/osworld_examples\",\n",
        "    \"--test_all_meta_path\", \"../test_data/osworld_examples/test_chrome_noisy_10.json\",\n",
        "    \"--result_dir\", str(results_dir_chrome_noisy),\n",
        "]\n",
        "\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        cmd,\n",
        "        cwd=ARPO_ROOT / \"OSWorld\",\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=3600  # 60 min timeout (10 tasks √ó ~4.5 min + buffer)\n",
        "    )\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"‚úÖ Evaluation complete in {elapsed/60:.1f} minutes!\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Show last 50 lines\n",
        "    print(\"\\nüìä Last 50 lines of output:\")\n",
        "    print(\"\\n\".join(result.stdout.split(\"\\n\")[-50:]))\n",
        "    \n",
        "    if result.returncode != 0:\n",
        "        print(f\"\\n‚ö†Ô∏è  Warning: Process returned code {result.returncode}\")\n",
        "        print(\"Last 20 lines of stderr:\")\n",
        "        print(\"\\n\".join(result.stderr.split(\"\\n\")[-20:]))\n",
        "        \n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"‚ùå Evaluation timed out after 60 minutes\")\n",
        "    print(\"‚ö†Ô∏è  Some tasks may have completed - check results folder\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. View Results for Noisy Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "print(\"\\nüîç Analyzing NOISY CHROME task results...\\n\")\n",
        "chrome_noisy_results = analyze_results(results_dir_chrome_noisy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if chrome_results and chrome_noisy_results:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä CHROME: Original vs Noisy Comparison\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    orig_avg = sum(r[\"score\"] for r in chrome_results) / len(chrome_results)\n",
        "    noisy_avg = sum(r[\"score\"] for r in chrome_noisy_results) / len(chrome_noisy_results)\n",
        "    \n",
        "    orig_success = sum(1 for r in chrome_results if r[\"score\"] >= 0.9) / len(chrome_results) * 100\n",
        "    noisy_success = sum(1 for r in chrome_noisy_results if r[\"score\"] >= 0.9) / len(chrome_noisy_results) * 100\n",
        "    \n",
        "    print(f\"\\nOriginal Chrome Tasks ({len(chrome_results)}):\")\n",
        "    print(f\"  Average Score:  {orig_avg:.3f}\")\n",
        "    print(f\"  Success Rate:   {orig_success:.1f}%\")\n",
        "    print(f\"  Passed:         {sum(1 for r in chrome_results if r['score'] >= 0.9)}/{len(chrome_results)}\")\n",
        "    \n",
        "    print(f\"\\nNoisy Chrome Tasks ({len(chrome_noisy_results)}):\")\n",
        "    print(f\"  Average Score:  {noisy_avg:.3f}\")\n",
        "    print(f\"  Success Rate:   {noisy_success:.1f}%\")\n",
        "    print(f\"  Passed:         {sum(1 for r in chrome_noisy_results if r['score'] >= 0.9)}/{len(chrome_noisy_results)}\")\n",
        "    \n",
        "    print(f\"\\nRobustness (Noisy/Original):\")\n",
        "    print(f\"  Score Ratio:    {noisy_avg/orig_avg:.2%}\" if orig_avg > 0 else \"  Score Ratio:  N/A\")\n",
        "    print(f\"  Success Ratio:  {noisy_success/orig_success:.2%}\" if orig_success > 0 else \"  Success Ratio:  N/A\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Expected performance (from ARPO paper)\n",
        "    print(\"\\nüìö Expected Performance (from ARPO paper):\")\n",
        "    print(\"  ARPO UITARS1.5 7B on OSWorld Chrome: ~22.6% success rate\")\n",
        "    print(\"  (10 tasks is a good sample for initial testing)\")\n",
        "    print(\"=\"*70)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Run cells 4 and 6 first to get results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Inspect Individual Task Trajectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def view_trajectory(result_dir, task_id):\n",
        "    \"\"\"View detailed trajectory for a specific task\"\"\"\n",
        "    traj_file = None\n",
        "    for f in Path(result_dir).rglob(f\"{task_id}/traj.jsonl\"):\n",
        "        traj_file = f\n",
        "        break\n",
        "    \n",
        "    if not traj_file:\n",
        "        print(f\"‚ö†Ô∏è  Trajectory not found for {task_id}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\nüìù Trajectory: {task_id}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    with open(traj_file) as f:\n",
        "        steps = [json.loads(line) for line in f]\n",
        "    \n",
        "    for i, step in enumerate(steps, 1):\n",
        "        print(f\"\\nStep {i}:\")\n",
        "        if \"prediction\" in step:\n",
        "            pred = step[\"prediction\"]\n",
        "            if isinstance(pred, str):\n",
        "                # Show first 200 chars\n",
        "                print(f\"  Prediction: {pred[:200]}...\")\n",
        "            else:\n",
        "                print(f\"  Prediction: {pred}\")\n",
        "        if \"action\" in step:\n",
        "            print(f\"  Action: {step['action']}\")\n",
        "        if \"reward\" in step:\n",
        "            print(f\"  Reward: {step['reward']}\")\n",
        "        if \"done\" in step:\n",
        "            print(f\"  Done: {step['done']}\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "\n",
        "# Example: View first task from Chrome results\n",
        "if chrome_results:\n",
        "    first_task = chrome_results[0][\"task_id\"]\n",
        "    print(f\"\\nüîç Viewing trajectory for first Chrome task: {first_task}\")\n",
        "    view_trajectory(results_dir_chrome, first_task)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No results available yet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Restore Original Agent Configuration\n",
        "\n",
        "After evaluation, restore the agent to use localhost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Restore backup\n",
        "if backup_file.exists():\n",
        "    shutil.copy(backup_file, agent_file)\n",
        "    print(f\"‚úÖ Restored original agent configuration\")\n",
        "    print(f\"   Agent is now using localhost:9000 again\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No backup found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What This Notebook Does\n",
        "\n",
        "1. ‚úÖ Connects to Colab GPU server (via ngrok)\n",
        "2. ‚úÖ Runs 10 original Chrome tasks\n",
        "3. ‚úÖ Runs 10 noisy Chrome tasks\n",
        "4. ‚úÖ Analyzes results and computes success rates\n",
        "5. ‚úÖ Compares robustness (original vs noisy)\n",
        "6. ‚úÖ Restores original configuration\n",
        "\n",
        "### Results Location\n",
        "\n",
        "- **Original Chrome**: `results/gpu_eval_chrome_10/`\n",
        "- **Noisy Chrome**: `results/gpu_eval_chrome_noisy_10/`\n",
        "\n",
        "Each task folder contains:\n",
        "- `traj.jsonl` - Step-by-step log\n",
        "- `result.txt` - Final score (0.0 or 1.0)\n",
        "- `step_*.png` - Screenshots\n",
        "- `recording.mp4` - Video\n",
        "\n",
        "### Dataset Info\n",
        "\n",
        "- **Total tasks**: 128 across 10 domains\n",
        "- **Chrome tasks**: Testing first 10 (out of 18 available)\n",
        "- **Other domains available**: gimp (19), vs_code (19), libreoffice_calc (12), etc.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Analyze failed Chrome tasks to understand errors\n",
        "- Compare with ARPO paper results (~22.6% on full OSWorld)\n",
        "- Test other domains if needed\n",
        "- Use insights for further training/fine-tuning"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "arpo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
