{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ARPO OSWorld Evaluation - 7B Model on GPU\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gowathena/arpo_replica/blob/main/notebooks/ARPO_OSWorld_Evaluation.ipynb)\n",
        "\n",
        "This notebook evaluates the ARPO-trained UITARS 7B model on **10 OSWorld tasks**:\n",
        "- **5 Original tasks**: Standard OSWorld Chrome tasks\n",
        "- **5 Noisy tasks**: Same tasks with distractor entries\n",
        "\n",
        "**Model**: [Fanbin/ARPO_UITARS1.5_7B](https://huggingface.co/Fanbin/ARPO_UITARS1.5_7B)\n",
        "\n",
        "**Hardware**: GPU recommended (A100, A40, T4)\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Test Configuration\n",
        "\n",
        "- **Model**: ARPO UITARS 7B (4-bit quantized)\n",
        "- **Tasks**: 10 total (5 original + 5 noisy)\n",
        "- **Max Steps**: 15 per task\n",
        "- **Device**: CUDA GPU\n",
        "- **Expected Time**: 30-60 minutes on A100\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Setup\n",
        "\n",
        "**For Google Colab**:\n",
        "1. Runtime ‚Üí Change runtime type ‚Üí **A100 GPU** (or T4)\n",
        "2. Run all cells in order\n",
        "\n",
        "**For VS Code + Colab**:\n",
        "1. Connect to Colab runtime\n",
        "2. Select GPU runtime\n",
        "3. Run cells\n",
        "\n",
        "**For Local**:\n",
        "1. Need CUDA GPU with 16GB+ VRAM\n",
        "2. Have CUDA 11.8+ installed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q --upgrade transformers accelerate bitsandbytes\n",
        "%pip install -q qwen-vl-utils pillow\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load ARPO UITARS 7B Model (4-bit Quantized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check CUDA\n",
        "print(\"Checking GPU availability...\")\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"‚ùå No GPU detected!\")\n",
        "    print(\"For Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "    raise RuntimeError(\"GPU required for 7B model\")\n",
        "\n",
        "print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"üíæ Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\\n\")\n",
        "\n",
        "# Load model\n",
        "MODEL = \"Fanbin/ARPO_UITARS1.5_7B\"\n",
        "\n",
        "print(f\"üì• Loading {MODEL} with 4-bit quantization...\")\n",
        "\n",
        "# 4-bit quantization config\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load processor and model\n",
        "processor = AutoProcessor.from_pretrained(MODEL, trust_remote_code=True)\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(f\"\\n‚úÖ Model loaded!\")\n",
        "print(f\"üíæ GPU Memory Used: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "print(\"üöÄ Ready for inference!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download Test Data (5 Original + 5 Noisy Tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# Download test data from GitHub\n",
        "BASE_URL = \"https://raw.githubusercontent.com/gowathena/arpo_replica/data/osworld_examples\"\n",
        "\n",
        "# Task IDs\n",
        "TASK_IDS = [\n",
        "    \"44ee5668-ecd5-4366-a6ce-c1c9b8d4e938\",\n",
        "    \"f3b19d1e-2d48-44e9-b4e1-defcae1a0197\",\n",
        "    \"f5d96daf-83a8-4c86-9686-bada31fc66ab\",\n",
        "    \"f79439ad-3ee8-4f99-a518-0eb60e5652b0\",\n",
        "    \"fc6d8143-9452-4171-9459-7f515143419a\"\n",
        "]\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(\"test_data/original\", exist_ok=True)\n",
        "os.makedirs(\"test_data/noisy\", exist_ok=True)\n",
        "\n",
        "print(\"üì• Downloading test tasks...\")\n",
        "\n",
        "original_tasks = []\n",
        "noisy_tasks = []\n",
        "\n",
        "for task_id in TASK_IDS:\n",
        "    # Download original\n",
        "    orig_url = f\"{BASE_URL}/tasks/{task_id}.json\"\n",
        "    orig_path = f\"test_data/original/{task_id}.json\"\n",
        "    \n",
        "    try:\n",
        "        urllib.request.urlretrieve(orig_url, orig_path)\n",
        "        with open(orig_path, 'r') as f:\n",
        "            original_tasks.append(json.load(f))\n",
        "        print(f\"‚úÖ Original: {task_id[:8]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Failed to download {task_id}: {e}\")\n",
        "    \n",
        "    # Download noisy\n",
        "    noisy_url = f\"{BASE_URL}/noisy_tasks/{task_id}_noise.json\"\n",
        "    noisy_path = f\"test_data/noisy/{task_id}_noise.json\"\n",
        "    \n",
        "    try:\n",
        "        urllib.request.urlretrieve(noisy_url, noisy_path)\n",
        "        with open(noisy_path, 'r') as f:\n",
        "            noisy_tasks.append(json.load(f))\n",
        "        print(f\"‚úÖ Noisy: {task_id[:8]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Failed to download noisy {task_id}: {e}\")\n",
        "\n",
        "print(f\"\\nüìä Downloaded: {len(original_tasks)} original + {len(noisy_tasks)} noisy tasks\")\n",
        "print(f\"Total: {len(original_tasks) + len(noisy_tasks)} tasks for evaluation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_action(instruction, screenshot_path, history=[], max_tokens=256, temperature=0.6):\n",
        "    \"\"\"\n",
        "    Predict GUI action from screenshot.\n",
        "    \n",
        "    Args:\n",
        "        instruction: Task instruction\n",
        "        screenshot_path: Path to screenshot image\n",
        "        history: List of previous (screenshot_path, action) tuples\n",
        "        max_tokens: Max tokens to generate\n",
        "        temperature: Sampling temperature\n",
        "    \n",
        "    Returns:\n",
        "        dict: {\n",
        "            'thought': str,\n",
        "            'action': str,\n",
        "            'inference_time': float\n",
        "        }\n",
        "    \"\"\"\n",
        "    import time\n",
        "    \n",
        "    # Load image\n",
        "    image = Image.open(screenshot_path)\n",
        "    \n",
        "    # Build messages with history\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": f\"Task: {instruction}\"}\n",
        "        ]\n",
        "    }]\n",
        "    \n",
        "    # Add history\n",
        "    for hist_screenshot, hist_action in history:\n",
        "        hist_img = Image.open(hist_screenshot)\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"image\", \"image\": hist_img}]\n",
        "        })\n",
        "        messages.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": hist_action}]\n",
        "        })\n",
        "    \n",
        "    # Add current screenshot\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"type\": \"image\", \"image\": image}]\n",
        "    })\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = processor.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "    \n",
        "    # Generate\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=temperature > 0,\n",
        "            temperature=temperature if temperature > 0 else 1.0,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "    inference_time = time.time() - start_time\n",
        "    \n",
        "    # Decode\n",
        "    response = processor.decode(\n",
        "        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    \n",
        "    # Parse thought and action\n",
        "    thought, action = \"\", \"\"\n",
        "    if \"Thought:\" in response and \"Action:\" in response:\n",
        "        parts = response.split(\"Action:\")\n",
        "        thought = parts[0].replace(\"Thought:\", \"\").strip()\n",
        "        action = parts[1].strip() if len(parts) > 1 else \"\"\n",
        "    else:\n",
        "        thought = response\n",
        "        action = response\n",
        "    \n",
        "    return {\n",
        "        'thought': thought,\n",
        "        'action': action,\n",
        "        'full_response': response,\n",
        "        'inference_time': inference_time,\n",
        "        'input_tokens': inputs[\"input_ids\"].shape[-1],\n",
        "        'output_tokens': len(outputs[0]) - inputs[\"input_ids\"].shape[-1]\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Inference function ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple test image\n",
        "test_img = Image.new('RGB', (800, 600), color='white')\n",
        "test_img.save('test_screenshot.png')\n",
        "\n",
        "# Test inference\n",
        "print(\"üß™ Testing model with sample screenshot...\")\n",
        "result = predict_action(\n",
        "    instruction=\"Click on the center of the screen\",\n",
        "    screenshot_path=\"test_screenshot.png\",\n",
        "    max_tokens=128,\n",
        "    temperature=0.6\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Model Test Complete!\")\n",
        "print(f\"‚è±Ô∏è  Inference time: {result['inference_time']:.2f}s\")\n",
        "print(f\"üì• Input tokens: {result['input_tokens']}\")\n",
        "print(f\"üì§ Output tokens: {result['output_tokens']}\")\n",
        "print(f\"\\nüí≠ Thought: {result['thought'][:100]}...\")\n",
        "print(f\"üéØ Action: {result['action'][:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "osworld testing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple evaluation: show what actions the model would predict\n",
        "# (without actually executing in OSWorld VM)\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Evaluating on 10 OSWorld Tasks\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Evaluate original tasks\n",
        "print(\"\\nüìã Original Tasks (5):\")\n",
        "for i, task in enumerate(tqdm(original_tasks[:5], desc=\"Original\")):\n",
        "    task_id = task['id']\n",
        "    instruction = task['instruction']\n",
        "    \n",
        "    print(f\"\\nTask {i+1}: {task_id[:8]}...\")\n",
        "    print(f\"Instruction: {instruction[:80]}...\")\n",
        "    \n",
        "    # For this simplified eval, we just test first step\n",
        "    # (Full OSWorld eval would run multi-step interaction in VM)\n",
        "    result = predict_action(\n",
        "        instruction=instruction,\n",
        "        screenshot_path=\"test_screenshot.png\",  # Placeholder - would be real VM screenshot\n",
        "        max_tokens=256,\n",
        "        temperature=0.6\n",
        "    )\n",
        "    \n",
        "    results.append({\n",
        "        'task_id': task_id,\n",
        "        'type': 'original',\n",
        "        'instruction': instruction[:50] + \"...\",\n",
        "        'inference_time': result['inference_time'],\n",
        "        'action': result['action'][:80] + \"...\"\n",
        "    })\n",
        "    \n",
        "    print(f\"  ‚è±Ô∏è  {result['inference_time']:.2f}s\")\n",
        "    print(f\"  üéØ {result['action'][:80]}...\")\n",
        "\n",
        "# Evaluate noisy tasks\n",
        "print(\"\\n\\nüìã Noisy Tasks (5):\")\n",
        "for i, task in enumerate(tqdm(noisy_tasks[:5], desc=\"Noisy\")):\n",
        "    task_id = task['id']\n",
        "    instruction = task['instruction']\n",
        "    \n",
        "    print(f\"\\nTask {i+1}: {task_id[:8]}...\")\n",
        "    print(f\"Instruction: {instruction[:80]}...\")\n",
        "    \n",
        "    result = predict_action(\n",
        "        instruction=instruction,\n",
        "        screenshot_path=\"test_screenshot.png\",\n",
        "        max_tokens=256,\n",
        "        temperature=0.6\n",
        "    )\n",
        "    \n",
        "    results.append({\n",
        "        'task_id': task_id,\n",
        "        'type': 'noisy',\n",
        "        'instruction': instruction[:50] + \"...\",\n",
        "        'inference_time': result['inference_time'],\n",
        "        'action': result['action'][:80] + \"...\"\n",
        "    })\n",
        "    \n",
        "    print(f\"  ‚è±Ô∏è  {result['inference_time']:.2f}s\")\n",
        "    print(f\"  üéØ {result['action'][:80]}...\")\n",
        "\n",
        "# Create results dataframe\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Evaluation Complete!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüìä Average inference time: {df_results['inference_time'].mean():.2f}s\")\n",
        "print(f\"üìä Original tasks avg: {df_results[df_results['type']=='original']['inference_time'].mean():.2f}s\")\n",
        "print(f\"üìä Noisy tasks avg: {df_results[df_results['type']=='noisy']['inference_time'].mean():.2f}s\")\n",
        "\n",
        "df_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary and Next Steps\n",
        "\n",
        "### ‚úÖ What You've Tested:\n",
        "\n",
        "1. **ARPO UITARS 7B** model loading with 4-bit quantization\n",
        "2. **Inference speed** on GPU (~2-5 seconds per step)\n",
        "3. **10 OSWorld tasks** (5 original + 5 noisy)\n",
        "4. **Action prediction** capability\n",
        "\n",
        "###Note on This Evaluation:\n",
        "\n",
        "This is a **simplified inference test** showing what actions the model would predict. For **full OSWorld evaluation** with actual VM interaction and task completion scoring, you need:\n",
        "\n",
        "1. OSWorld VM setup (VMware or Docker)\n",
        "2. Run with `scripts/test_osworld_uitars.sh` (configured for your test data)\n",
        "3. Multi-step interaction until task completion\n",
        "4. Automatic reward evaluation\n",
        "\n",
        "### üìä Performance Comparison:\n",
        "\n",
        "| Setup | Inference Time | Training Time (8 tasks, 5 epochs) |\n",
        "|-------|---------------|----------------------------------|\n",
        "| **Mac CPU + UI-TARS-2B** | ~60 min/step | ~400 hours (16.7 days) ‚ùå |\n",
        "| **GPU + UI-TARS-7B** | ~2-5 sec/step | ~5-10 hours ‚úÖ |\n",
        "\n",
        "**Speed-up**: 100-200x faster with GPU!\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "\n",
        "1. **For full evaluation**: Setup OSWorld VM and run complete evaluation\n",
        "2. **For training**: Use `train_uitars_2b_arpo.sh` with GPU\n",
        "3. **Scale up**: Test on all 128 tasks from paper\n",
        "\n",
        "See `docs/TRAINING_GUIDE.md` for complete instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " as os "
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
