{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UI-TARS 7B GPU Server for Mac OSWorld\n",
        "\n",
        "## ğŸ¯ Purpose\n",
        "\n",
        "Run UI-TARS 7B on **Colab A100 GPU** and connect from your **Mac OSWorld** (already tested and working!).\n",
        "\n",
        "## Architecture & Data Flow\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    Complete Workflow                              â”‚\n",
        "â”‚                                                                   â”‚\n",
        "â”‚  Colab A100 GPU (This Notebook)        Mac (Your Computer)       â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
        "â”‚  â”‚ UI-TARS 7B Model         â”‚         â”‚ OSWorld VM (VMware)  â”‚  â”‚\n",
        "â”‚  â”‚ Flask Server :9000       â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚\n",
        "â”‚  â”‚ + ngrok tunnel            â”‚         â”‚ â”‚ Ubuntu Desktop   â”‚ â”‚  â”‚\n",
        "â”‚  â”‚                           â”‚         â”‚ â”‚ Chrome/Firefox   â”‚ â”‚  â”‚\n",
        "â”‚  â”‚ 1. Receives screenshot    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚ Captures screen  â”‚ â”‚  â”‚\n",
        "â”‚  â”‚ 2. Generates action       â”‚         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚\n",
        "â”‚  â”‚ 3. Returns prediction    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                      â”‚  â”‚\n",
        "â”‚  â”‚                           â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚\n",
        "â”‚  â”‚ âš ï¸ NO ACCESS TO RESULTS   â”‚         â”‚ â”‚ Executes action  â”‚ â”‚  â”‚\n",
        "â”‚  â”‚ (Just inference service)  â”‚         â”‚ â”‚ Saves results     â”‚ â”‚  â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”‚ - traj.jsonl      â”‚ â”‚  â”‚\n",
        "â”‚                                        â”‚ â”‚ - result.txt      â”‚ â”‚  â”‚\n",
        "â”‚                                        â”‚ â”‚ - screenshots/    â”‚ â”‚  â”‚\n",
        "â”‚                                        â”‚ â”‚ - recording.mp4   â”‚ â”‚  â”‚\n",
        "â”‚                                        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚\n",
        "â”‚                                        â”‚                      â”‚  â”‚\n",
        "â”‚                                        â”‚ ğŸ“ results/gpu_eval/ â”‚  â”‚\n",
        "â”‚                                        â”‚ (Saved on YOUR Mac)  â”‚  â”‚\n",
        "â”‚                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "Key Points:\n",
        "âœ… Colab GPU = Inference only (no file access)\n",
        "âœ… Mac OSWorld = Task execution + result storage\n",
        "âœ… Full precision model (no quantization) for best quality\n",
        "âœ… Results saved locally: results/gpu_eval/*/traj.jsonl\n",
        "```\n",
        "\n",
        "**Advantage**: Uses your TESTED Mac setup, just with GPU model (1200x faster!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m142.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hâœ… Dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install -q --upgrade transformers accelerate\n",
        "%pip install -q qwen-vl-utils pillow flask pyngrok\n",
        "\n",
        "print(\"âœ… Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure ngrok Authentication\n",
        "\n",
        "Set up your ngrok authtoken for stable tunnels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ngrok authenticated!\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import conf\n",
        "import getpass\n",
        "\n",
        "# Set your ngrok authtoken\n",
        "# Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "print(\"ğŸ“ Enter your ngrok authtoken (from https://dashboard.ngrok.com/get-started/your-authtoken):\")\n",
        "NGROK_AUTH_TOKEN = getpass.getpass(\"Authtoken: \")\n",
        "\n",
        "if not NGROK_AUTH_TOKEN or len(NGROK_AUTH_TOKEN) < 20:\n",
        "    raise ValueError(\"Invalid authtoken. Please get it from https://dashboard.ngrok.com\")\n",
        "\n",
        "conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "print(\"âœ… ngrok authenticated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load ARPO UITARS 7B Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… GPU: NVIDIA A100-SXM4-40GB\n",
            "ğŸ’¾ Memory: 39.6 GB\n",
            "\n",
            "ğŸ“¥ Loading Fanbin/ARPO_UITARS1.5_7B (full precision)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "820b63d8284e4e8181413bfe9629fb7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/763 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdd44b881a4a48edb98313d55d6737f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e008b9d96665446a8fa2e0bef79b2b51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b77294105ce44009a00e494e1640e8d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92030bfec47745b8ac09d33a2e9deea8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "775225c8870a407ca48d1397d1696828",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a063a180f474fb8b6e7194090b4b296",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e482850c283548ca9d7dbf527b02e49f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9edce2cfa9bf476296865dc604cd6f60",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6bcdc0e90af434094784d796bdb405c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27d4fd285c1b43979e9965efced3b4c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6db3d4c58adc4753a28170bfdc0dc48e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ff557551ff941eb871baafa1f50f351",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.70G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51cee89b401243b8a1e8d4070a61ba66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32c3a66158b94621a5937ac7d8d9175e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8781146a884468ba07641cc3a4da6af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Model loaded!\n",
            "ğŸ’¾ GPU Memory: 14.43 GB\n",
            "ğŸ” Model type: Qwen2_5_VLModel\n",
            "ğŸš€ Ready for serving!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"âŒ No GPU! Go to: Runtime â†’ Change runtime type â†’ A100 GPU\")\n",
        "    raise RuntimeError(\"GPU required\")\n",
        "\n",
        "print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"ğŸ’¾ Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\\n\")\n",
        "\n",
        "# Model\n",
        "MODEL = \"Fanbin/ARPO_UITARS1.5_7B\"\n",
        "print(f\"ğŸ“¥ Loading {MODEL} (full precision)...\")\n",
        "\n",
        "# Load model (use AutoModelForVision2Seq for generation capabilities)\n",
        "processor = AutoProcessor.from_pretrained(MODEL, trust_remote_code=True)\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    MODEL,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,  # Use bfloat16 for better performance on A100\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(f\"\\nâœ… Model loaded!\")\n",
        "print(f\"ğŸ’¾ GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "print(f\"ğŸ” Model type: {type(model).__name__}\")\n",
        "print(f\"âœ… Has generate(): {hasattr(model, 'generate')}\")\n",
        "print(\"ğŸš€ Ready for serving!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Flask server created!\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import time\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/v1/chat/completions', methods=['POST'])\n",
        "def chat_completions():\n",
        "    try:\n",
        "        data = request.json\n",
        "        messages = data.get('messages', [])\n",
        "        max_tokens = min(data.get('max_tokens', 256), 512)\n",
        "        temperature = data.get('temperature', 0.6)\n",
        "        \n",
        "        # Process messages\n",
        "        model_messages = []\n",
        "        for msg in messages:\n",
        "            if msg['role'] == 'system':\n",
        "                continue\n",
        "            \n",
        "            content = msg.get('content', [])\n",
        "            if isinstance(content, str):\n",
        "                content = [{\"type\": \"text\", \"text\": content}]\n",
        "            \n",
        "            processed_content = []\n",
        "            for item in content:\n",
        "                if item['type'] == 'image_url':\n",
        "                    image_url = item['image_url']['url']\n",
        "                    if image_url.startswith('data:image'):\n",
        "                        base64_data = image_url.split(',')[1]\n",
        "                        image_data = base64.b64decode(base64_data)\n",
        "                        image = Image.open(BytesIO(image_data))\n",
        "                        processed_content.append({\"type\": \"image\", \"image\": image})\n",
        "                else:\n",
        "                    processed_content.append(item)\n",
        "            \n",
        "            model_messages.append({\"role\": msg['role'], \"content\": processed_content})\n",
        "        \n",
        "        # Generate\n",
        "        inputs = processor.apply_chat_template(\n",
        "            model_messages,\n",
        "            add_generation_prompt=True,\n",
        "            tokenize=True,\n",
        "            return_dict=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(model.device)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                do_sample=temperature > 0,\n",
        "                temperature=temperature if temperature > 0 else 1.0,\n",
        "            )\n",
        "        \n",
        "        response_text = processor.decode(\n",
        "            outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "        \n",
        "        inference_time = time.time() - start_time\n",
        "        print(f\"[{time.strftime('%H:%M:%S')}] Generated in {inference_time:.2f}s\")\n",
        "        \n",
        "        return jsonify({\n",
        "            \"id\": f\"chatcmpl-{int(time.time())}\",\n",
        "            \"object\": \"chat.completion\",\n",
        "            \"model\": \"arpo-uitars-7b\",\n",
        "            \"choices\": [{\n",
        "                \"index\": 0,\n",
        "                \"message\": {\"role\": \"assistant\", \"content\": response_text},\n",
        "                \"finish_reason\": \"stop\"\n",
        "            }],\n",
        "            \"usage\": {\n",
        "                \"prompt_tokens\": inputs[\"input_ids\"].shape[-1],\n",
        "                \"completion_tokens\": len(outputs[0]) - inputs[\"input_ids\"].shape[-1]\n",
        "            },\n",
        "            \"inference_time\": inference_time\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/v1/models', methods=['GET'])\n",
        "def list_models():\n",
        "    return jsonify({\"object\": \"list\", \"data\": [{\"id\": \"arpo-uitars-7b\"}]})\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\"status\": \"healthy\", \"model\": \"arpo-uitars-7b\"})\n",
        "\n",
        "print(\"âœ… Flask server created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import threading\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Configure port\n",
        "port = 9001\n",
        "\n",
        "# Disconnect any existing ngrok tunnels (safe on Colab)\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    time.sleep(1)\n",
        "    print(\"âœ… Cleaned up old ngrok tunnels\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Start Flask server in background thread\n",
        "print(\"ğŸš€ Starting Flask server on port 9001...\")\n",
        "\n",
        "def run_flask():\n",
        "    try:\n",
        "        app.run(host='0.0.0.0', port=port, debug=False, use_reloader=False, threaded=True)\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Flask error: {e}\")\n",
        "\n",
        "flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
        "flask_thread.start()\n",
        "\n",
        "# Give Flask time to start\n",
        "time.sleep(3)\n",
        "\n",
        "# Open ngrok tunnel\n",
        "print(\"ğŸŒ Creating ngrok tunnel...\")\n",
        "public_url = ngrok.connect(port).public_url\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… SERVER IS READY!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nğŸ“ Public URL: {public_url}\")\n",
        "print(f\"ğŸ“ API Endpoint: {public_url}/v1/chat/completions\")\n",
        "print(f\"ğŸ“ Health Check: {public_url}/health\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\\nğŸ”§ COPY THIS URL FOR YOUR MAC:\")\n",
        "print(f'\\nGPU_SERVER_URL = \"{public_url}\"')\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\\nâš ï¸  Keep this cell running! Don't stop or restart.\")\n",
        "print(\"âš ï¸  Minimize this tab and leave it open.\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Keep alive - simple loop\n",
        "print(\"\\nğŸ“Š Server is running. Waiting for requests...\")\n",
        "print(\"(You'll see inference logs here as requests come in)\\n\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(10)  # Check every 10 seconds instead of 1\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\nğŸ›‘ Server stopped by user.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n\\nâŒ Server crashed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Instructions\n",
        "\n",
        "### On Colab (This Notebook):\n",
        "1. âœ… Run cells 1-5\n",
        "2. âœ… Cell 5 will keep running (server active)\n",
        "3. âœ… Copy the `GPU_SERVER_URL` from cell 5 output\n",
        "\n",
        "### On Your Mac (Terminal or Jupyter):\n",
        "\n",
        "**Option 1: Use the evaluation notebook**\n",
        "```python\n",
        "# In ARPO_OSWorld_Evaluation.ipynb Cell 2:\n",
        "GPU_SERVER_URL = \"YOUR_URL_FROM_ABOVE\"  # Paste from Colab\n",
        "```\n",
        "\n",
        "**Option 2: Run from terminal**\n",
        "```bash\n",
        "cd /Users/hanszhu/Desktop/ARPO_replicate\n",
        "\n",
        "# Update agent\n",
        "sed -i '' 's|http://localhost:9000|YOUR_URL_FROM_ABOVE|g' OSWorld/mm_agents/uitars_agent.py\n",
        "\n",
        "# Run evaluation\n",
        "cd OSWorld\n",
        "python run_uitars.py \\\n",
        "    --headless \\\n",
        "    --observation_type screenshot \\\n",
        "    --max_steps 15 \\\n",
        "    --model arpo-uitars-7b \\\n",
        "    --temperature 0.6 \\\n",
        "    --test_all_meta_path ../test_data/osworld_examples/test_10tasks.json \\\n",
        "    --result_dir ../results/gpu_eval/\n",
        "```\n",
        "\n",
        "### Expected Performance (Actual Test Results):\n",
        "- **Inference**: ~2-5 seconds per step (model only)\n",
        "- **Per task**: ~4-5 minutes (includes VM overhead, network latency, execution)\n",
        "- **5 tasks**: 22m 40s total\n",
        "- **10 tasks**: ~45-50 minutes estimated\n",
        "\n",
        "### Monitor:\n",
        "- Watch this cell's output for inference logs\n",
        "- Each request shows: \"[HH:MM:SS] Generated in X.XXs\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
