{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UI-TARS 7B GPU Server for Mac OSWorld\n",
        "\n",
        "## üéØ Purpose\n",
        "\n",
        "Run UI-TARS 7B on **Colab A100 GPU** and connect from your **Mac OSWorld** (already tested and working!).\n",
        "\n",
        "## Architecture & Data Flow\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    Complete Workflow                              ‚îÇ\n",
        "‚îÇ                                                                   ‚îÇ\n",
        "‚îÇ  Colab A100 GPU (This Notebook)        Mac (Your Computer)       ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
        "‚îÇ  ‚îÇ UI-TARS 7B Model         ‚îÇ         ‚îÇ OSWorld VM (VMware)  ‚îÇ  ‚îÇ\n",
        "‚îÇ  ‚îÇ Flask Server :9000       ‚îÇ         ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ\n",
        "‚îÇ  ‚îÇ + ngrok tunnel            ‚îÇ         ‚îÇ ‚îÇ Ubuntu Desktop   ‚îÇ ‚îÇ  ‚îÇ\n",
        "‚îÇ  ‚îÇ                           ‚îÇ         ‚îÇ ‚îÇ Chrome/Firefox   ‚îÇ ‚îÇ  ‚îÇ\n",
        "‚îÇ  ‚îÇ 1. Receives screenshot    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ ‚îÇ Captures screen  ‚îÇ ‚îÇ  ‚îÇ\n",
        "‚îÇ  ‚îÇ 2. Generates action       ‚îÇ         ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ\n",
        "‚îÇ  ‚îÇ 3. Returns prediction    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ                      ‚îÇ  ‚îÇ\n",
        "‚îÇ  ‚îÇ                           ‚îÇ         ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ\n",
        "‚îÇ  ‚îÇ ‚ö†Ô∏è NO ACCESS TO RESULTS   ‚îÇ         ‚îÇ ‚îÇ Executes action  ‚îÇ ‚îÇ  ‚îÇ\n",
        "‚îÇ  ‚îÇ (Just inference service)  ‚îÇ         ‚îÇ ‚îÇ Saves results     ‚îÇ ‚îÇ  ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ ‚îÇ - traj.jsonl      ‚îÇ ‚îÇ  ‚îÇ\n",
        "‚îÇ                                        ‚îÇ ‚îÇ - result.txt      ‚îÇ ‚îÇ  ‚îÇ\n",
        "‚îÇ                                        ‚îÇ ‚îÇ - screenshots/    ‚îÇ ‚îÇ  ‚îÇ\n",
        "‚îÇ                                        ‚îÇ ‚îÇ - recording.mp4   ‚îÇ ‚îÇ  ‚îÇ\n",
        "‚îÇ                                        ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ\n",
        "‚îÇ                                        ‚îÇ                      ‚îÇ  ‚îÇ\n",
        "‚îÇ                                        ‚îÇ üìÅ results/gpu_eval/ ‚îÇ  ‚îÇ\n",
        "‚îÇ                                        ‚îÇ (Saved on YOUR Mac)  ‚îÇ  ‚îÇ\n",
        "‚îÇ                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "Key Points:\n",
        "‚úÖ Colab GPU = Inference only (no file access)\n",
        "‚úÖ Mac OSWorld = Task execution + result storage\n",
        "‚úÖ Results saved locally: results/gpu_eval/*/traj.jsonl\n",
        "‚úÖ Colab doesn't need to access results (it's just a model API)\n",
        "```\n",
        "\n",
        "**Advantage**: Uses your TESTED Mac setup, just with GPU model (1200x faster!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q --upgrade transformers accelerate bitsandbytes\n",
        "!pip install -q qwen-vl-utils pillow flask pyngrok\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure ngrok Authentication\n",
        "\n",
        "Set up your ngrok authtoken for stable tunnels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyngrok import conf\n",
        "\n",
        "# Set your ngrok authtoken\n",
        "# Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "NGROK_AUTH_TOKEN = \"38TrgHgInoFMcWQqSsVsxh6bdnT_6LakZy5EGZi9ax97eWGTH\"\n",
        "\n",
        "conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "print(\"‚úÖ ngrok authenticated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load ARPO UITARS 7B Model (4-bit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"‚ùå No GPU! Go to: Runtime ‚Üí Change runtime type ‚Üí A100 GPU\")\n",
        "    raise RuntimeError(\"GPU required\")\n",
        "\n",
        "print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"üíæ Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\\n\")\n",
        "\n",
        "# Model\n",
        "MODEL = \"Fanbin/ARPO_UITARS1.5_7B\"\n",
        "print(f\"üì• Loading {MODEL} with 4-bit quantization...\")\n",
        "\n",
        "# 4-bit config\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load\n",
        "processor = AutoProcessor.from_pretrained(MODEL, trust_remote_code=True)\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(f\"\\n‚úÖ Model loaded!\")\n",
        "print(f\"üíæ GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "print(\"üöÄ Ready for serving!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import time\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/v1/chat/completions', methods=['POST'])\n",
        "def chat_completions():\n",
        "    try:\n",
        "        data = request.json\n",
        "        messages = data.get('messages', [])\n",
        "        max_tokens = min(data.get('max_tokens', 256), 512)\n",
        "        temperature = data.get('temperature', 0.6)\n",
        "        \n",
        "        # Process messages\n",
        "        model_messages = []\n",
        "        for msg in messages:\n",
        "            if msg['role'] == 'system':\n",
        "                continue\n",
        "            \n",
        "            content = msg.get('content', [])\n",
        "            if isinstance(content, str):\n",
        "                content = [{\"type\": \"text\", \"text\": content}]\n",
        "            \n",
        "            processed_content = []\n",
        "            for item in content:\n",
        "                if item['type'] == 'image_url':\n",
        "                    image_url = item['image_url']['url']\n",
        "                    if image_url.startswith('data:image'):\n",
        "                        base64_data = image_url.split(',')[1]\n",
        "                        image_data = base64.b64decode(base64_data)\n",
        "                        image = Image.open(BytesIO(image_data))\n",
        "                        processed_content.append({\"type\": \"image\", \"image\": image})\n",
        "                else:\n",
        "                    processed_content.append(item)\n",
        "            \n",
        "            model_messages.append({\"role\": msg['role'], \"content\": processed_content})\n",
        "        \n",
        "        # Generate\n",
        "        inputs = processor.apply_chat_template(\n",
        "            model_messages,\n",
        "            add_generation_prompt=True,\n",
        "            tokenize=True,\n",
        "            return_dict=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(model.device)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                do_sample=temperature > 0,\n",
        "                temperature=temperature if temperature > 0 else 1.0,\n",
        "            )\n",
        "        \n",
        "        response_text = processor.decode(\n",
        "            outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "        \n",
        "        inference_time = time.time() - start_time\n",
        "        print(f\"[{time.strftime('%H:%M:%S')}] Generated in {inference_time:.2f}s\")\n",
        "        \n",
        "        return jsonify({\n",
        "            \"id\": f\"chatcmpl-{int(time.time())}\",\n",
        "            \"object\": \"chat.completion\",\n",
        "            \"model\": \"arpo-uitars-7b\",\n",
        "            \"choices\": [{\n",
        "                \"index\": 0,\n",
        "                \"message\": {\"role\": \"assistant\", \"content\": response_text},\n",
        "                \"finish_reason\": \"stop\"\n",
        "            }],\n",
        "            \"usage\": {\n",
        "                \"prompt_tokens\": inputs[\"input_ids\"].shape[-1],\n",
        "                \"completion_tokens\": len(outputs[0]) - inputs[\"input_ids\"].shape[-1]\n",
        "            },\n",
        "            \"inference_time\": inference_time\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/v1/models', methods=['GET'])\n",
        "def list_models():\n",
        "    return jsonify({\"object\": \"list\", \"data\": [{\"id\": \"arpo-uitars-7b\"}]})\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\"status\": \"healthy\", \"model\": \"arpo-uitars-7b\"})\n",
        "\n",
        "print(\"‚úÖ Flask server created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import threading\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Configure port\n",
        "port = \"9000\"\n",
        "\n",
        "# Start Flask server in background thread\n",
        "def run_flask():\n",
        "    app.run(host='0.0.0.0', port=int(port), debug=False, use_reloader=False)\n",
        "\n",
        "flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
        "flask_thread.start()\n",
        "\n",
        "print(\"Starting Flask server...\")\n",
        "time.sleep(3)\n",
        "\n",
        "# Open ngrok tunnel\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üåê SERVER IS READY!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüìç Public URL: {public_url}\")\n",
        "print(f\"üìç API Endpoint: {public_url}/v1/chat/completions\")\n",
        "print(f\"üìç Health Check: {public_url}/health\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\\nüîß COPY THIS URL FOR YOUR MAC:\")\n",
        "print(f'\\nGPU_SERVER_URL = \"{public_url}\"')\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\\n‚ö†Ô∏è  Keep this cell running! Server will stop if interrupted.\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Keep alive\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\nServer stopped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Instructions\n",
        "\n",
        "### On Colab (This Notebook):\n",
        "1. ‚úÖ Run cells 1-5\n",
        "2. ‚úÖ Cell 5 will keep running (server active)\n",
        "3. ‚úÖ Copy the `GPU_SERVER_URL` from cell 5 output\n",
        "\n",
        "### On Your Mac (Terminal or Jupyter):\n",
        "\n",
        "**Option 1: Use the evaluation notebook**\n",
        "```python\n",
        "# In ARPO_OSWorld_Evaluation.ipynb Cell 2:\n",
        "GPU_SERVER_URL = \"YOUR_URL_FROM_ABOVE\"  # Paste from Colab\n",
        "```\n",
        "\n",
        "**Option 2: Run from terminal**\n",
        "```bash\n",
        "cd /Users/hanszhu/Desktop/ARPO_replicate\n",
        "\n",
        "# Update agent\n",
        "sed -i '' 's|http://localhost:9000|YOUR_URL_FROM_ABOVE|g' OSWorld/mm_agents/uitars_agent.py\n",
        "\n",
        "# Run evaluation\n",
        "cd OSWorld\n",
        "python run_uitars.py \\\n",
        "    --headless \\\n",
        "    --observation_type screenshot \\\n",
        "    --max_steps 15 \\\n",
        "    --model arpo-uitars-7b \\\n",
        "    --temperature 0.6 \\\n",
        "    --test_all_meta_path ../test_data/osworld_examples/test_10tasks.json \\\n",
        "    --result_dir ../results/gpu_eval/\n",
        "```\n",
        "\n",
        "### Expected Performance:\n",
        "- **Inference**: ~2-5 seconds per step\n",
        "- **Per task**: ~30-75 seconds (15 steps)\n",
        "- **10 tasks**: ~5-12 minutes total\n",
        "\n",
        "### Monitor:\n",
        "- Watch this cell's output for inference logs\n",
        "- Each request shows: \"[HH:MM:SS] Generated in X.XXs\""
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
